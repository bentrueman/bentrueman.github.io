[
  {
    "path": "posts/2024-07-09-estimating-correlation-matrices-when-the-data-include-missing-and-left-censored-values/",
    "title": "Estimating correlation matrices when the data include missing and left-censored values",
    "description": "A one-step Bayesian approach.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2024-07-09",
    "categories": [],
    "contents": "\nI recently put together a GitHub repository with some collected tips on how to fit common models in environmental science when the data are partially left-censored. This occurs most often when observations are reported as falling below a detection limit. One of these models is used to estimate a correlation matrix, and I thought I’d explore that here using a simulation. I follow the typical approach of simulating data, fitting a model, and then comparing the parameter values that generated the data to the model estimates.\nWe’ll need the following R packages and options:\n\n\nlibrary(\"MASS\") # random generation for multivariate normal\nlibrary(\"tidyverse\") # data wrangling\nlibrary(\"scales\") # rescaling vectors\nlibrary(\"withr\") # set a random seed\nlibrary(\"trialr\") # generate random correlation matrices\nlibrary(\"brms\") # generate Stan code template\nlibrary(\"bgamcar1\") # customize Stan code and fit models\nlibrary(\"cmdstanr\") # postprocess model\nlibrary(\"posterior\") # postprocess model\nlibrary(\"ggplot2\") # visualize\nlibrary(\"patchwork\") # compose plots\noptions(mc.cores = parallel::detectCores()) # detect and set number of CPU cores\n\n\n\n\n\nFirst, let’s set the simulation parameters. I’m choosing a missing rate of 20% and a censoring rate of approximately 35% for each variable.\n\n\nfilename_prefix <- \"correlation-matrix-simulation-\" # for saving models\nn_variables <- 5\nn_observations <- 100\nn_simulations <- 25\nproportion_missing <- 0.2\nn_missing <- round(proportion_missing * n_observations)\n\n# this is the proportion of sigma to subtract from the mean, yielding the censoring threshold:\nproportion_censored <- 0.25\n\n# parameters for lognormal distributions of mu and sigma:\nmean_mu <- 0\nsd_mu <- 1\nmeanlog_sigma <- 1\nsdlog_sigma <- 1\n\n# make variable/indicator names:\nthese_variables <- paste0(\"x\", seq(n_variables))\nthese_indicators <- paste0(\"cens_x\", seq(n_variables))\n\nthis_seed <- 124256764 # choose a random seed:\n\n\nThen, let’s simulate the data. Missingness is not random: higher values are more likely to be missing.\n\n\nwith_seed(this_seed, {\n\n  simulation_inputs <- replicate(n_simulations, {\n    tibble(\n      variable = these_variables,\n      mu = rnorm(n_variables, mean_mu, sd_mu),\n      sigma = rlnorm(n_variables, meanlog_sigma, sdlog_sigma),\n      censoring_thresholds = mu - proportion_censored * sigma\n    )\n  }, simplify = FALSE)\n\n  correlation_matrices <- replicate(n_simulations, {\n    trialr::rlkjcorr(1, n_variables)\n  }, simplify = FALSE)\n\n  data <- map2(correlation_matrices, simulation_inputs, \\(x, y) {\n\n    sigma <- diag(y$sigma)\n\n    covariance_matrix <- sigma %*% x %*% sigma\n\n    data <- mvrnorm(n = n_observations, mu = y$mu, Sigma = covariance_matrix)\n\n    # add in missings and censored:\n\n    data_missing <- data |>\n      apply(2, \\(u) {u[sample(seq(n_observations), n_missing, prob = rescale(u))] <- NA; u})\n\n\n    censoring_indicators <- data_missing |>\n      sweep(2, y$censoring_thresholds, FUN = \"<\") |>\n      apply(2, \\(u) if_else(u, \"left\", \"none\")) |>\n      apply(2, \\(u) replace_na(u, \"none\"))\n\n    data_censored <- data_missing |>\n      sweep(2, y$censoring_thresholds, FUN = pmax)\n\n    # convert to tibble:\n\n    colnames(data_censored) <- these_variables\n\n    colnames(censoring_indicators) <- these_indicators\n\n    bind_cols(data.frame(data_censored), data.frame(censoring_indicators)) |>\n      as_tibble()\n\n  })\n})\n\n\nNow that we have data, we can verify that the proportion censored is reasonable:\n\n\ndata |>\n  list_rbind(names_to = \"simulation\") |>\n  summarize(across(starts_with(\"cens_\"), ~ mean(.x == \"left\")))\n\n# A tibble: 1 × 5\n  cens_x1 cens_x2 cens_x3 cens_x4 cens_x5\n    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   0.352   0.362   0.344   0.348    0.35\n\nWe’ll fit the models using bgamcar1, a package I built to fill a few gaps in the functionality of the immensely useful brms. First let’s generate the model formula:\n\n\nthese_formulas <- lapply(these_variables, \\(x) bf(paste0(x, \" | mi() ~ 1\")))\nmultivariate_formula <- mvbrmsformula(flist = these_formulas) + set_rescor(TRUE)\n\n\nNow we can do the sampling. After it’s done, we’ll save CSVs of the draws and read them back in as CmdStanFit objects for easy post-processing.\n\n\n\n\n\nmap(seq_along(data), ~ fit_stan_model(\n    file = paste0(\"models/\", filename_prefix, .x),\n    seed = this_seed,\n    bform = multivariate_formula,\n    bdata = data[[.x]],\n    car1 = FALSE,\n    var_xcens = these_variables,\n    cens_ind = these_indicators,\n    lcl = simulation_inputs[[.x]]$censoring_thresholds,\n    family = \"gaussian\",\n    backend = \"cmdstanr\",\n    overwrite = TRUE\n  ))\n# read the CSVs storing the draws as cmdstanfit objects:\nfilenames <- map_chr(seq(n_simulations), ~ paste0(\"models/\", filename_prefix, .x)) |>\n    map(~ paste0(.x, \"-\", 1:4, \".csv\"))\ncensored_model_fitted <- map(filenames, as_cmdstan_fit)\n\n\nNot all of these models fully converged. Usually, that would mean diagnosing and tweaking each one, but here we’ll just identify them and check later whether or not they recover the true parameter values well. In particular, we’ll flag any model where the maximum tree depth was exceeded, divergent transitions occurred, the estimated Bayesian fraction of missing information was less than 0.3, r-hat values were greater than 1.05, or effective sample size for any parameter was less than 400—see here for details.\n\n\ndiagnostics_1 <- map(censored_model_fitted, ~ .x$diagnostic_summary()) |>\n  map_lgl(\n    ~ all(.x$num_divergent == 0) & all(.x$num_max_treedepth == 0) & all(.x$ebfmi >= 0.3)\n  )\n\ndiagnostics_2 <- map(censored_model_fitted, ~ .x$summary()) |>\n  map_lgl(\n    ~ with(.x,\n           all(rhat < 1.05, na.rm = TRUE) &\n             all(ess_bulk >= 400, na.rm = TRUE) &\n             all(ess_tail >= 400, na.rm = TRUE))\n  )\n\ndiagnostics <- tibble(simulation = seq(n_simulations), converged = diagnostics_1 & diagnostics_2)\n\n\nTo evaluate model performance, we’ll need to collect the true values from all the simulations…\n\n\nthese_variable_pairs <- data.frame(t(combn(these_variables, 2))) |>\n  unite(col = variable, X1, X2)\n\ncorrelation_matrices_tbl <- correlation_matrices |>\n  map(\\(x) x[lower.tri(x)]) |>\n  map(\\(x) mutate(these_variable_pairs, rho = x)) |>\n  list_rbind(names_to = \"simulation\")\n\nsimulation_inputs_tbl <- simulation_inputs |>\n  list_rbind(names_to = \"simulation\")\n\n\n… and compare them to the model estimates:\n\n\nmodel_draws <- censored_model_fitted |>\n  map(~ .x$draws(format = \"draws_df\")) |>\n  map(as_tibble) |>\n  list_rbind(names_to = \"simulation\") |> \n  # rename correlation parameters:\n  rename_with(\n    .cols = matches(\"^Rescor\\\\[\\\\d\\\\,\\\\d\\\\]\"), \n    .fn = ~ str_replace(.x, \"(\\\\d+),(\\\\d+)\", \"x\\\\1,x\\\\2\")\n  )\n\nestimates <- list(\n  mu_estimated = \"Intercept$\", sigma_estimated = \"^sigma\", rho_estimated = \"^Rescor\\\\[x\\\\d,x\\\\d\\\\]\"\n) |>\n  map2(\n    list(simulation_inputs_tbl, simulation_inputs_tbl, correlation_matrices_tbl),\n    ~ model_draws |>\n      select(c(simulation, matches(.x))) |>\n      pivot_longer(-simulation, names_to = \"param\") |> \n      mutate(\n        variable = str_extract_all(param, paste(these_variables, collapse = \"|\")),\n        variable = map(variable, ~ paste(.x, collapse = \"_\")),\n        variable = unlist(variable)\n      ) |> \n      group_by(variable, simulation) |> \n      summarize(\n        lower = quantile(value, .025),\n        upper = quantile(value, .975),\n        estimate = quantile(value, .5)\n      ) |>\n      ungroup() |>\n      right_join(.y, by = c(\"simulation\", \"variable\")) |>\n      left_join(diagnostics, by = \"simulation\")\n  )\n\n\nThat’s it! Now we just need to put all the information in a set of plots.\n\n\n\n\n\n\nThe model is recovering our parameters reasonably well. But how does it compare to a simpler approach? Let’s try estimating correlation coefficients with the stats::cor() function. We’ll use pairwise complete cases for calculating each of the correlation coefficients and impute the censoring limits for the left-censored values.\n\n\nestimates_conventional <- data |> \n  map(~ select(.x, starts_with(\"x\"))) |> \n  map(~ cor(.x, use = \"pairwise\")) |> \n  map(\\(x) x[lower.tri(x)]) |>\n  map(\\(x) mutate(these_variable_pairs, rho = x)) |>\n  list_rbind(names_to = \"simulation\") |> \n  left_join(\n    correlation_matrices_tbl, \n    by = c(\"simulation\", \"variable\"),\n    suffix = c(\"_estimate\", \"_true\")\n  )\n\n\n\n\n\nThe conventional approach does comparably well at estimating small correlations, but it tends to underestimate large correlations—especially large negative ones. The Bayesian version also has the advantage that a robust model is straightforward—if extreme observations occur, we can compensate by using a student t likelihood, whereas stats::cor() doesn’t have that option.\n\n\n\n",
    "preview": "posts/2024-07-09-estimating-correlation-matrices-when-the-data-include-missing-and-left-censored-values/estimating-correlation-matrices-when-the-data-include-missing-and-left-censored-values_files/figure-html5/combine-plots-1.png",
    "last_modified": "2024-07-11T09:18:00-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-07-06-probabilistic-principal-component-analysis-for-censored-data/",
    "title": "Probabilistic principal component analysis for censored data",
    "description": "Application of Bayesian PCA to a heavily left-censored (simulated) dataset.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2023-07-06",
    "categories": [],
    "contents": "\nPrincipal component analysis (PCA) has been a recent focus in my efforts to find Bayesian versions of statistical methods that play nicely with censored data. The solution in this case is probabilistic PCA (Bishop 1998), demonstrated here using a simulated dataset and then fitted using Stan via the R interface cmdstanr. The simulation approach and Stan program borrow heavily from this Jupyter notebook—I (roughly) translate the Python to R and modify the simulation and Stan program to allow imputation of censored values as a step in the model fitting process.\nProbabilistic PCA is a latent variable model that can be written as follows:\n\\[\ny \\sim N(Wz, \\sigma^2I) \\\\\nz \\sim N(0, I)\n\\]\nwhere \\(y\\) is an \\(n \\times d\\) matrix of data, \\(z\\) is an \\(n \\times k\\) matrix of latent variables with \\(k \\leq d\\), \\(W\\) is a \\(d \\times k\\) transformation matrix mapping from the latent space to the data space, and \\(\\sigma^2\\) is the error variance.\nTo explore this model, we’ll need the following packages:\n\n\nlibrary(\"tidyverse\")\nlibrary(\"here\")\nlibrary(\"cmdstanr\")\nlibrary(\"MASS\", include.only = \"mvrnorm\")\nlibrary(\"pracma\", include.only = \"orth\")\nlibrary(\"withr\")\n\n\n\n\n\nFirst, we need to simulate from the data-generating process:\n\n\nn <- 500 # number of observations\nd <- 2 # number of data dimensions\nk <- 1 # number of latent space dimensions\nsigma2 <- 0.05 # error variance\n\nwith_seed(124563, {\n  # sample latent variable Z:\n  Z <- mvrnorm(n, mu = rep(0, k), Sigma = diag(1, k, k)) # n x k\n  # create orthogonal transformation matrix:\n  W <- orth(matrix(runif(d * k), d, k)) # d x k\n  # generate additive noise:\n  noise <- mvrnorm(n, mu = rep(0, d), Sigma = sigma2 * diag(1, d, d)) # n x d\n})\n\n# generate data:\nY <- W %*% t(Z) + t(noise)\n\n\nHere are the data, along with the first principal component—that is, the first and only column of \\(W\\).\n\n\n\nNext, we’ll need to put the data into a list for passing to cmdstanr methods.\n\n\nstandata <- list(\n  N = n,\n  D = d,\n  K = k,\n  y = Y\n)\nstanseed <- 215678\n\n\nRead the Stan program and compile (the Stan code is available in the Jupyter notebook linked at the top of this post; I’ve modified the basic version only slightly).\n\n\nppca_scode <- readLines(here(\"ppca.stan\"))\nmodel <- cmdstan_model(stan_file = write_stan_file(ppca_scode))\n\n\nOnce compiling is done, sample from the posterior:\n\n\nfit <- model$sample(data = standata, seed = stanseed, parallel_chains = 4)\nfit$save_output_files(\n  dir = \"models\", basename = \"ppca\", random = FALSE, timestamp = FALSE\n)\n\n\n\n\n\nThe model recovers the transformation matrix quite well:\n\n\n# extract draws:\ndraws_df <- fit$draws(format = \"df\")\n\n# extract the raw transformation matrix:\nA <- draws_df %>% \n  select(starts_with(\"A\"))\n\n# calculate the posterior mean and orthogonalize:\nW_est <- A %>% \n  apply(2, \\(x) mean(abs(x))) %>% \n  matrix(standata$D, standata$K) %>% \n  orth()\n\n\n\n\n\nSame goes for the error variance:\n\n\nmean(draws_df$sigma ^ 2)\n\n[1] 0.04871603\n\nTo compare the latent variable estimates with the true values, we have to correct for the sign of \\(W\\), iteration-by-iteration:\n\n\n# extract the signs of transformation matrix:\nS <- tibble(sign = apply(sign(A), 1, unique)) \n# extract raw latent variables:\nX <- draws_df %>% \n  select(starts_with(\"x\"))\n# sign correction:\nZ_est <- X %>% \n  apply(2, \\(x) x * S$sign) %>% \n  apply(2, mean) %>% \n  matrix(standata$N, standata$K) %>% \n  `colnames<-`(paste0(\"pc\", seq_len(standata$K))) %>% \n  as_tibble()\n\n\nHere are the posterior means of the latent variables compared with the true \\(z\\):\n\n\n\nThis is very similar to the PCA solution:\n\n\npca <- princomp(t(Y))\npca$loadings[,1] # principal component directions\n\n[1] 0.6589110 0.7522209\n\nW_est[,1] # transformation matrix\n\n[1] 0.6591170 0.7520404\n\n\n\n# correlation between PC scores and latent variable:\ncor(Z_est, pca$scores[,1])\n\n         [,1]\npc1 0.9999959\n\nNow let’s turn to the problem of censoring. First, let’s modify the simulated data to introduce left-censoring:\n\n\n# set censoring limits:\nlcl_1 <- -1.5 # left-censoring limit, y1\nlcl_2 <- 0 # left-censoring limit, y2\n\n# simulate left-censored data:\nY_cens <- Y\nY_cens[1, ] <- pmax(Y[1, ], lcl_1)\nY_cens[2, ] <- pmax(Y[2, ], lcl_2)\n\n# censored indicators:\nnondetect_1 <- Y[1,] < lcl_1\nnondetect_2 <- Y[2,] < lcl_2\n\n\nValues censored in one dimension are represented as blue lines and values censored in both dimensions are represented as points within the shaded region:\n\n\n\nWe’ll need a new list of data inputs to pass to cmdstan functions:\n\n\nstandata_cens <- list(\n  N = n,\n  D = d,\n  K = k,\n  y = Y_cens,\n  Ncens_y1 = sum(nondetect_1),\n  Ncens_y2 = sum(nondetect_2),\n  Jcens_y1 = seq_len(n)[nondetect_1],\n  Jcens_y2 = seq_len(n)[nondetect_2],\n  U_y1 = lcl_1,\n  U_y2 = lcl_2\n)\n\n\n\n\n\nThe following Stan program estimates the latent variable model after imputing censored data:\n\n// modified from: https://github.com/nbip/PPCA-Stan/blob/master/PPCA.ipynb\n\ndata {\n  int<lower=1> N; // num datapoints\n  int<lower=1> D; // num data-space dimensions\n  int<lower=1> K; // num latent space dimensions\n  array[D, N] real y;\n  // data for censored imputation:\n  int<lower=0> Ncens_y1; // number of censored, y1\n  int<lower=0> Ncens_y2; // number of censored, y2\n  array[Ncens_y1] int<lower=1> Jcens_y1; // positions of censored, y1\n  array[Ncens_y2] int<lower=1> Jcens_y2; // positions of censored, y2\n  real U_y1; // left-censoring limit, y1\n  real U_y2; // left-censoring limit, y2\n}\ntransformed data {\n  matrix[K, K] Sigma; // identity matrix\n  vector<lower=0>[K] diag_elem;\n  vector<lower=0>[K] zr_vec; // zero vector\n  for (k in 1 : K) {\n    zr_vec[k] = 0;\n  }\n  for (k in 1 : K) {\n    diag_elem[k] = 1;\n  }\n  Sigma = diag_matrix(diag_elem);\n}\nparameters {\n  matrix[D, K] A; // transformation matrix / PC loadings\n  array[N] vector[K] x; // latent variables\n  real<lower=0> sigma; // noise variance\n  // censored value parameters:\n  array[Ncens_y1] real<upper=U_y1> Ycens_y1; // estimated censored, y1\n  array[Ncens_y2] real<upper=U_y2> Ycens_y2; // estimated censored, y2\n}\ntransformed parameters {\n  // combine observed with estimated censored:\n  array[D, N] real yl = y;\n  yl[1, Jcens_y1] = Ycens_y1;\n  yl[2, Jcens_y2] = Ycens_y2;\n}\nmodel {\n  for (i in 1 : N) {\n    x[i] ~ multi_normal(zr_vec, Sigma);\n  } // zero-mean, identity matrix\n  for (i in 1 : N) {\n    for (d in 1 : D) {\n      //y[d,i] ~ normal(dot_product(row(A, d), x[i]), sigma);\n      target += normal_lpdf(yl[d, i] | dot_product(row(A, d), x[i]), sigma);\n    }\n  }\n}\ngenerated quantities {\n  vector[N] log_lik;\n  for (n in 1 : N) {\n    for (d in 1 : D) {\n      log_lik[n] = normal_lpdf(yl[d, n] | dot_product(row(A, d), x[n]), sigma);\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\n\nAs above, we sample and extract the components. This time, the PCA solution is quite biased, while the Bayesian implementation does much better:\n\n\npca_cens <- princomp(t(Y_cens))\npca_cens$loadings[,1] # PCA solution\n\n[1] 0.8476363 0.5305778\n\nW_est_cens[,1] # Bayesian solution\n\n[1] 0.6601378 0.7511445\n\nW[,1] # true W\n\n[1] 0.6646687 0.7471383\n\n\n\n\n\n\n\nHere are the posterior means of the latent variables compared with the true \\(z\\); again, there is bias at the low end of the conventional principal component scores, but not with the Bayesian version.\n\n\n\n\n\n\nBishop, Christopher. 1998. “Bayesian PCA.” In Advances in Neural Information Processing Systems, edited by M. Kearns, S. Solla, and D. Cohn. Vol. 11. MIT Press. https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf.\n\n\n\n\n",
    "preview": "posts/2023-07-06-probabilistic-principal-component-analysis-for-censored-data/probabilistic-principal-component-analysis-for-censored-data_files/figure-html5/plot-pc-cens-1.png",
    "last_modified": "2023-07-07T10:58:03-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-06-20-a-bayesian-approach-to-predicting-disinfection-byproduct-formation-using-regularizing-priors/",
    "title": "A Bayesian approach to predicting disinfection byproduct formation using regularizing priors",
    "description": "Using heirarchical shrinkage priors on linear regression coefficients to reduce the variance of ccorrelated predictors.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2023-06-20",
    "categories": [],
    "contents": "\nI’ve recently been experimenting with regularizing priors in Bayesian linear regression models, and I thought a good opportunity to explore them was to revisit an older paper of mine featuring a \\(p>N\\) problem (Trueman et al. 2016). The paper described predicting the byproducts of chlorine disinfection in drinking water using fluorescence over a range of excitation wavelengths.\nOne of the benefits of a Bayesian approach here, I think, is that any prediction performance metric can be calculated or each posterior draw—meaning that it too has a distribution. I think this really helps add some perspective to comparisons among models that point estimates of predictive error lack.\nFor this post, I used linear regression with a Gamma likelihood to keep predictions positive and a heirarchical shrinkage prior on the regression coefficients (Piironen and Vehtari 2017) to help stabilize the estimates of correlated predictors. The \\(n\\) trihalomethane concentrations, \\(y_n\\), are predicted using the following model, and fitted with brms.\n\\[\ny_n \\sim Gamma(\\mu, \\alpha)~\\text{[likelihood]} \\\\\nlog(\\mu) = \\gamma + X \\beta~\\text{[model for }\\mu] \\\\\n\\text{priors:} \\\\\n\\gamma \\sim T(4.1,2.5,3)~\\text{[intercept]} \\\\\n\\alpha \\sim Gamma(0.01, 0.01)~\\text{[shape parameter]} \\\\\n\\beta_j \\sim N(0, \\tau^2\\widetilde{\\lambda}_j^2)~\\text{[regression coefficients]} \\\\\n\\widetilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2\\lambda_j^2} \\\\\n\\lambda_j \\sim T(0, 1, 3),~j=1,...,D \\\\\nc^2 \\sim \\text{Inv-Gamma}(\\nu/2, \\nu s^2/2) \\\\\n\\tau \\sim C^+(0,1) \\\\\n\\nu = 4 \\\\\ns^2 = 4 \\\\\n\\]\nwhere \\(T\\), \\(C^+\\), \\(Gamma\\), and \\(N\\) are the student-t, half-Cauchy, gamma, and normal distributions. The parameters \\(\\mu\\) and \\(\\alpha\\) represent the mean and shape of the gamma distribution, while \\(\\gamma\\) and \\(\\beta\\) represent the intercept and the regression coefficients; \\(X\\) is an \\(n \\times D\\) matrix of \\(D\\) scaled predictors.\nThe horseshoe prior on the regression coefficients is a little more difficult to understand. The basic intuition is that the global shrinkage parameter, \\(\\tau\\), pulls all of the regression coefficients towards zero, while the local shrinkage parameters, \\(\\widetilde{\\lambda}_j\\), allow some coefficients to escape the shrinkage with their fat-tailed prior. The \\(\\widetilde{\\lambda}_j\\) are a function of \\(\\tau\\), \\(\\lambda_j\\), and \\(c^2\\), each of which have their own prior distributions. For regression coefficients with large absolute values, the prior approaches \\(N(0,c^2)\\) while for the smallest coefficients, the prior approaches \\(N(0,\\tau^2\\lambda_j^2)\\) (Piironen and Vehtari 2017).\nTo fit the model, we’ll need the following packages:\n\n\nlibrary(\"tidyverse\")\nlibrary(\"brms\")\nlibrary(\"withr\")\nlibrary(\"here\")\nlibrary(\"ggdist\", include.only = \"median_qi\")\nlibrary(\"rstan\", include.only = \"get_sampler_params\")\nlibrary(\"data.table\", include.only = \"fread\")\nlibrary(\"posterior\", include.only = \"summarise_draws\")\n\n\n\n\n\nFirst, read the data using data.table::fread, which will be faster than readr::read_csv.\n\n\nplant <- fread(here(\"data-clean/dataset-plant.csv\")) %>% \n  as_tibble()\n\n\nThen, we’ll need functions to prepare the data and fit the model. The first one separates the training and test data, ranks the predictors in the training data by correlation with the response, chooses the \\(D\\) predictors with the highest correlation, fits the model, and generates the test predictions.\n\n\ndbp_model <- function(data, folds, this_fold, n, filename, ...) {\n  # generate train/test indices:\n  test_rows <- folds == this_fold\n  train_rows <- folds != this_fold\n  # prepare training data:\n  data_train <- data[train_rows, ]\n  # select the n variables in the training data having the largest \n  # correlation with the response:\n  corvars_train <- apply(\n    data_train, 2, \n    \\(x) suppressWarnings(cor(data_train[,\"thm_ppb\"], x))\n  )\n  these_vars <- names(sort(abs(corvars_train), decreasing  = TRUE)[seq_len(n + 1)])\n  data_train_sub <- select(data_train, all_of(these_vars)) %>% \n    scale()\n  # prepare test data:\n  data_test <- data[test_rows, ] %>% \n    select(all_of(these_vars), -thm_ppb) %>% \n    scale_testdata(data_train_sub)\n  # rescale training response:\n  data_train_sub[,\"thm_ppb\"] <- data_train$thm_ppb\n  # train model:\n  model_train <- fit_dbp_model(data_train_sub, this_fold, filename, ...)\n  # test model:\n  preds_test <- posterior_epred(\n    model_train, newdata = data_test\n  ) \n  list(\n    model_train = model_train, \n    preds_test = preds_test, \n    thm_ppb = data$thm_ppb[test_rows]\n  )\n}\n\n\nWe’ll need two helper functions as well: one as a wrapper around brms::brm…\n\n\nfit_dbp_model <- function(data, this_fold, filename, ...) {\n  brm(\n    thm_ppb ~ .,\n    data = data,\n    family = Gamma(link = \"log\"),\n    cores = 4,\n    backend = \"cmdstanr\",\n    file = here(paste0(\"models/thm-\", filename, \"-\", this_fold)),\n    file_refit = \"on_change\",\n    seed = 1578, \n    ...\n  )\n}\n\n\n… and one to scale the test data using the column means and standard deviations from the training data.\n\n\nscale_testdata <- function(data_test, data_train) {\n  trainscale <- list(\n    mean = attr(data_train, \"scaled:center\")[-1], \n    sd = attr(data_train, \"scaled:scale\")[-1]\n  ) %>% \n  map(\n    ~ matrix(\n      .x, \n      nrow = nrow(data_test), \n      ncol = ncol(data_test),\n      byrow = TRUE\n    )\n  )\n  (data_test - trainscale$mean) / trainscale$sd\n}\n\n\nBefore using these functions, some minor cleaning of the data is necessary:\n\n\nthmfp <- plant %>% \n  filter(!is.na(thm_ppb)) %>%\n  mutate(formation = if_else(formation == \"T\", 1, 0)) %>% \n  select(-haa_ppb) %>%\n  rename_with(.fn = ~ str_replace(.x, \"^(\\\\d)\", \"f\\\\1\"), .cols = everything())\n\n\nThen, determine random fold assignment as follows. For each of the \\(k=10\\) folds, we fit the data on the \\(k-1\\) training folds and make test predictions using the \\(k^{th}\\) fold.\n\n\nfolds <- with_seed(125656, sample(1:10, nrow(thmfp), replace = TRUE))\nstopifnot(all(1:10 %in% folds))\n\n\nThis yields a minimum training set for each iteration of 30 observations. Now we can run the model; I’m using \\(D=38\\) predictors, making this a \\(p>n\\) problem without generating an overly large model.\n\n\ncv_out <- sort(unique(folds)) %>%\n  set_names() %>%\n  map(~ dbp_model(\n    thmfp, folds, this_fold = .x, n = 38,\n    filename = \"horseshoe\",\n    prior = set_prior(horseshoe(df = 3)),\n    control = list(adapt_delta = .999, max_treedepth = 12)\n  ))\n\n\n\n\n\nWe can assess prediction performance using the median absolute prediction error…\n\n\nmae <- cv_out %>% \n  map(~ apply(.x$preds_test, 1, \\(y) abs(y - .x$thm_ppb))) %>% \n  reduce(rbind) %>% \n  apply(2, median) %>% \n  median_qi() %>% \n  mutate(across(starts_with(\"y\"), ~ round(.x, 1)))\n\n\n… and plot the predictions against the observations to assess the fit.\n\n\n\nHere are the regression coefficients mapped onto a contour plot representing the average fluorescence intensity at each excitation/emission pair. The coordinates most correlated with the response tend to cluster at the boundaries of the region characteristic of humic acid fluorescence.\n\n\n\nThe out-of-sample prediction error had a 95% credible interval of 11.9–24.4 \\(\\mu\\)g L-1. This is probably too high to be useful in practice, but more data would likely help—the 38 observations used here don’t make for a large training set.\n\n\n\nPiironen, Juho, and Aki Vehtari. 2017. “Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors.” Electronic Journal of Statistics 11 (2). https://doi.org/10.1214/17-EJS1337SI.\n\n\nTrueman, Benjamin F., Sean A. MacIsaac, Amina K. Stoddart, and Graham A. Gagnon. 2016. “Prediction of Disinfection by-Product Formation in Drinking Water via Fluorescence Spectroscopy.” Environmental Science: Water Research & Technology 2 (2): 383–89. https://doi.org/10.1039/C5EW00285K.\n\n\n\n\n",
    "preview": "posts/2023-06-20-a-bayesian-approach-to-predicting-disinfection-byproduct-formation-using-regularizing-priors/a-bayesian-approach-to-predicting-disinfection-byproduct-formation-using-regularizing-priors_files/figure-html5/eem-plot-1.png",
    "last_modified": "2023-07-12T12:25:46-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2023-03-13-regression-models-with-censored-predictors/",
    "title": "Regression models with censored predictors",
    "description": "Another modification of brms-generated Stan code, this time to fit regression models with censored predictors.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2023-03-13",
    "categories": [],
    "contents": "\nRecently, I’ve encountered several regression problems where the predictors are partially censored. To tackle this issue, I turned to Paul Bürkner’s excellent package for Bayesian regression, brms (Bürkner 2017). Paul has written a helpful vignette on handling missing values. The general strategy we’ll use here is outlined in that vignette: imputation during model fitting by declaring the missing values as parameters. That is, we’ll fit a multivariate model that imputes missing values in both the predictor and the response variable. The only problem is that brms doesn’t handle censored predictors, so we’ll need to customize our approach a little bit.\nLet’s start by loading the following packages:\n\n\nlibrary(\"tidyverse\")\nlibrary(\"glue\")\nlibrary(\"ggdist\")\nlibrary(\"withr\")\nlibrary(\"brms\")\nlibrary(\"rstan\")\noptions(mc.cores = parallel::detectCores()) # for parallel MCMC chains\n\n\n\n\n\nThen we’ll need to simulate some data:\n\n\nn <- 50 # total number of observations\nlcl <- -1 # lower censoring limit\n\nwith_seed(1235, {\n  data <- tibble(\n    x = rnorm(n, 0, 2),\n    y = 2 * x + rnorm(n)\n  )\n})\n\ndata$x_star <- data$x\ndata$x_star[c(3, 29)] <- NA # add missing values\ndata$cens_x_star <- replace_na(data$x_star < lcl, FALSE) # censoring indicator\ndata$x_star <- pmax(data$x_star, lcl) # left-censor values\n\n# train/test split:\ntrain <- 1:25 \ndata_train <- data[train, ]\ndata_test <- data[-train, ]\n\n\nLet’s plot the simulated data so we can see what we’re dealing with. Missing x values are shown as horizontal lines, and left-censored x values are shown as horizontal line segments extending from the left edge of the plot to the censoring limit of -1. The true values corresponding to the censored and missing observations are transparent.\n\n\n\nThe trick at this point is to treat left-censored x values as missing values with an upper bound equal to the censoring limit, as described in the Stan manual. But since brms doesn’t do this we’ll have to code it ourselves. We’ll need two functions: one to modify the data list and another to modify the code that brms passes to Stan:\n\n\nmodify_standata <- function(sdata, data, lcl, var) {\n  \n  if (length(lcl) != length(var)) stop(\"lengths of 'var' and 'lcl' must be equal\")\n  \n  varstan <- str_remove_all(var, \"_\")\n  \n  for(i in seq_len(length(var))) {\n    sdata[[paste0(\"Ncens_\", varstan[i])]] <- sum(data[[paste0(\"cens_\", var[i])]]) # number of left-censored\n    # positions of left-censored:\n    sdata[[paste0(\"Jcens_\", varstan[i])]] <- as.array(seq_len(nrow(data))[data[[paste0(\"cens_\", var[i])]]]) \n    sdata[[paste0(\"U_\", varstan[i])]] <- lcl[i] # left-censoring limit\n  }\n  \n  sdata\n}\n\nmodify_stancode <- function(scode, var) {\n  \n  var <- str_remove_all(var, \"_\")\n  \n  for(i in seq_len(length(var))) {\n    \n    # modifications to data block:\n    n_cens <- glue(\"int<lower=0> Ncens_{var[i]};  // number of left-censored\")\n    j_cens <- glue(\"int<lower=1> Jcens_{var[i]}[Ncens_{var[i]}];  // positions of left-censored\")\n    u <- glue(\"real U_{var[i]};  // left-censoring limit\")\n    # modifications to parameters block:\n    y_cens <- glue(\"vector<upper=U_{var[i]}>[Ncens_{var[i]}] Ycens_{var[i]};  // estimated left-censored\")\n    # modifications to model block:\n    yl <- glue(\"Yl_{var[i]}[Jcens_{var[i]}] = Ycens_{var[i]}; // add imputed left-censored values\")\n    \n    scode <- scode %>%\n      # modifications to data block:\n      str_replace(\n        \"(data \\\\{\\n(.|\\n)*?)(?=\\n\\\\})\",\n        paste(c(\"\\\\1\", n_cens, j_cens, u), collapse = \"\\n  \")\n      ) %>% \n      # modifications to parameters block:\n      str_replace(\n        \"(parameters \\\\{\\n(.|\\n)*?)(?=\\n\\\\})\",\n        paste(c(\"\\\\1\\n  \", y_cens), collapse = \"\")\n      ) %>% \n      # modifications to model block:\n      str_replace(\n        \"(model \\\\{\\n(.|\\n)*?)(?=\\n    mu_)\",\n        paste(c(\"\\\\1\\n    \", yl), collapse = \"\")\n      )\n    \n  }\n  \n  class(scode) <- \"brmsmodel\"\n  \n  scode\n    \n}\n\n\nWe’ll also need a function to impute missing and censored values for prediction using the fitted model:\n\n\nimpute <- function(data, model, var, mi = NULL, cens = NULL, id = NULL) {\n  \n  varstan <- str_remove_all(var, \"_\")\n  \n  censored <- as_draws_df(model) %>% \n    as_tibble() %>% \n    select(starts_with(paste0(\"Ycens_\", varstan))) %>% \n    t()\n  \n  if (!is.null(id)) {\n    censored <- censored[id, ]\n  }\n  \n  missing <- as_draws_df(model) %>% \n    as_tibble() %>% \n    select(starts_with(paste0(\"Ymi_\", varstan))) %>% \n    t()\n  \n  ndraws <- ncol(censored)\n  \n  map(\n    seq_len(ndraws),\n    \\(x) {\n      data_imputed <- data\n      if (!is.null(mi)) {data_imputed[mi, var] <- missing[, x]}\n      if (!is.null(cens)) {data_imputed[cens, var] <- censored[, x]}\n      data_imputed\n    }, .progress = TRUE\n  )\n  \n}\n\n\nNow we’re ready to set up and run the model. First we need a formula that defines a model for both y and x. Since we don’t have anything else to go on, the model for x is intercept only, but we should be able to do better than that in many real applications. The calls to mi() define missing values and how they’re handled during model fitting—see the brms vignette for details.\n\n\nbform <- bf(y | mi() ~ mi(x_star)) + \n  bf(x_star | mi() ~ 1) +\n  set_rescor(FALSE)\n\n\nNext, we modify the data and code passed to Stan…\n\n\nsdata <- make_standata(bform, data = data_train) %>% \n  modify_standata(data_train, lcl, \"x_star\")\nscode <- make_stancode(bform, data = data_train) %>% \n  modify_stancode(\"x_star\")\n\n\n… and fit (and save) the model using rstan::stan.\n\n\nstanseed <- 1257\nmodel_rstan <- stan(\n  model_code = scode,\n  data = sdata,\n  sample_file = \"model-censored-x\", # save model as CSVs\n  seed = stanseed\n)\n\n\nWe can load the fitted model later using the following:\n\n\nmodel_rstan <- read_stan_csv(list.files(pattern = \"model-censored-x_\"))\n\n\nNow we can generate posterior predictions along a regular sequence of predictor values, using the posterior draws for the slope and intercept:\n\n\npost_draws <- as_draws_df(model_rstan) # posterior draws:\n\nxnew <- seq(min(data$x), max(data$x), length.out = 25) # regular sequence of x-values\n\npost_imp <- map(\n  seq_len(nrow(post_draws)),\n  ~ xnew * post_draws$`bsp_y[1]`[.x] + post_draws$Intercept_y[.x]\n) %>%\n  do.call(rbind, .)\n\n\nHere is the fitted model—the shaded region represents a 95% credible interval on the posterior expectation.\n\n\n\nLet’s compare the parameter estimates from this model with those of a simple linear regression model that doesn’t account for censoring or missingness:\n\n\nmodel_naive <- brm(\n  y ~ x_star,\n  data = data_train,\n  file = \"model-naive\",\n  file_refit = \"on_change\"\n)\npost_draws_naive <- as_draws_df(model_naive)\n\n\nThe posterior draws of the censoring model match the true parameter values closely, while those of the naive model are a poor match:\n\n\n\nTo estimate prediction performance for the training data, we first impute the censored and missing values. This generates a list of imputed datasets as long as the number of posterior samples.\n\n\ndata_train_imputed <- impute(data_train, model_rstan, \"x_star\", sdata$Jmi_xstar, sdata$Jcens_xstar)\n\n\nThen, we can calculate RMSE for the training data by generating predictions using the imputed datasets and the posterior draws of the model parameters:\n\n\nrmse <- function(y, yhat) {\n  sqrt(mean((y - yhat) ^ 2))\n}\nrmse_train <- map2_dbl(\n  seq_len(nrow(post_draws)),\n  data_train_imputed,\n  \\(x, y) {\n    yhat <- y$x_star * post_draws$`bsp_y[1]`[x] + post_draws$Intercept_y[x]\n    rmse(y$y, yhat)\n  }\n)\nmedian_qi(rmse_train) # summarize\n\n          y      ymin     ymax .width .point .interval\n1 0.8905452 0.7783883 1.154363   0.95 median        qi\n\nWe can also estimate out-of-sample error by imputing missing values and generating posterior predictions. Imputing censored values in the test data, though, is slightly more complicated: since there is no guarantee that model predictions will fall below the censoring limit, we impute by refitting the model to the training set, augmented by the censored observations from the test set:\n\n\ndata_combined <- bind_rows(data_train, data_test[data_test$cens_x_star, c(\"x_star\", \"cens_x_star\")])\nsdata_combined <- make_standata(bform, data = data_combined) %>% \n  modify_standata(data_combined, lcl, \"x_star\")\n\n\nWe fit the same model as we did on the training data. The augmented model excludes all of the response values from the test set, though: these are imputed during model fitting but not used further.\n\n\nmodel_rstan_combined <- stan(\n  model_code = scode,\n  data = sdata_combined,\n  sample_file = \"model-censored-x-combined\",\n  seed = stanseed,\n  control = list(adapt_delta = .99)\n)\n\n\n\n\n\nAfter fitting, we impute the censored values, and fill in the missings by posterior prediction.\n\n\ndata_test_imputed <- impute(\n  data = data_test,\n  model = model_rstan_combined,\n  var = \"x_star\",\n  mi = NULL,\n  cens = seq_len(nrow(data_test))[data_test$cens_x_star],\n  id = 8:13\n) %>% \n  # impute missings via posterior prediction:\n  map2(\n    seq_len(nrow(post_draws)),\n    ~ .x %>% \n        mutate(\n          x_star = if_else(\n            is.na(x_star), \n            post_draws$Intercept_xstar[.y], \n            x_star\n          )\n        )\n  )\n\n\nThen, we generate test predictions…\n\n\npreds_test <- map2(\n  seq_len(nrow(post_draws)),\n  data_test_imputed,\n  ~ .y$x_star * post_draws$`bsp_y[1]`[.x] + post_draws$Intercept_y[.x]\n)\n\n\n… and use them to calculate the RMSE. Since the naive model uses complete cases only, we omit the row with a missing x value from the RMSE calculation for the censored model.\n\n\nrmse_test <- apply(\n  do.call(rbind, preds_test)[,-4], \n  1, \n  \\(x) rmse(x, data_test$y[-4])\n) %>% \n  median_qi()\n\nrmse_naive <- apply(\n  posterior_epred(model_naive, newdata = data_test)[ , -4],\n  1,\n  \\(x) rmse(x, data_test$y[-4])\n) %>% \n  median_qi()\n\n\nFinally, let’s compare the predictions:\n\n\n\nAnd that’s it! The linear regression model with censored predictors recovers the true parameter values well and yields better prediction performance than the simple linear regression model.\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n\n\n",
    "preview": "posts/2023-03-13-regression-models-with-censored-predictors/regression-models-with-censored-predictors_files/figure-html5/plot-model-1.png",
    "last_modified": "2023-03-14T18:19:18-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-01-building-a-continuous-time-autoregressive-model-in-brms-and-stan/",
    "title": "Building a continuous-time autoregressive model in brms",
    "description": "A simple modification of brms-generated Stan code to fit first-order autoregressive models to irregularly-spaced time series.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\nUPDATES:\n1. This material is now implemented in an R package, bgamcar1, focused on GAMs with CAR(1) errors as a quick fix until a CAR(1) option is added to brms.\n2. This post has been updated to reflect the changes to prior specification implemented in brms 2.17.0.\nIn recent posts, I’ve written about using brms to fit fully Bayesian autoregressive models to left-censored time series data. And while this approach is powerful, it is not easy to use for irregularly spaced series.\nIt turns out, though, that there is a straightforward generalization of the first-order autoregressive—AR(1)—model, called the continuous-time AR(1), or CAR(1). Whereas the AR(1) takes the form\n\\[x_t = \\phi x_{t-1} + \\epsilon_t\\]\n(where \\(x\\) is the time series, \\(t\\) is time, \\(\\phi\\) defines the autocorrelation structure, and \\(\\epsilon_t\\) is an independent error term), the CAR(1) model has the following autocorrelation structure:\n\\[h(s, \\phi) = \\phi^s, s\\geq0, \\phi\\geq0 \\]\nwhere \\(s\\) is a real number representing the time difference between successive observations. This model is implemented in R by the function nlme::corCAR1() (Pinheiro et al. 2021), and there is an open issue on GitHub discussing implementation in brms.\nNot being able to wait for a future version of brms with CAR(1) as an option, I modified the brms generated Stan code to fit a CAR(1) model, as follows.\nFirst, let’s simulate a couple of irregularly-spaced AR(1) processes:\n\n\nstan_seed <- 1256\n\nphi <- .75\np_ret <- .6 # proportion retained\n\nwithr::with_seed(stan_seed, {\n  data <- tibble(\n    x = 1:100,\n    y1 = arima.sim(list(ar = phi), length(x)),\n    y2 = arima.sim(list(ar = phi), length(x))\n  ) %>% \n    pivot_longer(starts_with(\"y\"), names_to = \"g\", values_to = \"y\")\n  \n  subset <- data %>% \n    slice_sample(prop = p_ret) %>% \n    arrange(g, x) %>% \n    group_by(g) %>% \n    mutate(\n      x_lag = lag(x),\n      d_x = replace_na(x - x_lag, 0) # spacing of observations\n    ) %>% \n    ungroup()\n})\n\n\nThen, use brms to generate Stan code and the accompanying data as a list:\n\n\npriors <- prior(normal(.5, .25), class = ar, lb = 0, ub = 1)\nformula <- bf(y ~ ar(time = x, gr = g))\n\nsdata <- brms::make_standata(formula, prior = priors, data = subset)\nsdata$s <- subset$d_x # CAR(1) exponent \n\nscode <- brms::make_stancode(formula, prior = priors, data = subset)\n\n\nNext, modify the Stan code to fit a CAR(1) model…\n\n\nscode_car1 <- scode %>% \n  # add time difference variable s:\n  str_replace(\n    \"response variable\\\\\\n\", \n    \"response variable\\n  vector[N] s;  // CAR(1) exponent\\n\"\n  ) %>% \n  # set lower bound of zero on ar param:\n  str_replace(\n    \"vector\\\\[Kar\\\\] ar;\", \n    \"vector<lower=0>[Kar] ar;\"\n  ) %>% \n  # convert AR process to CAR1:\n  str_replace(\n    \"mu\\\\[n\\\\] \\\\+= Err\\\\[n, 1:Kar\\\\] \\\\* ar;\", \n    \"mu[n] += Err[n, 1] * pow(ar[1], s[n]); // CAR(1)\"\n  ) \n\nclass(scode_car1) <- \"brmsmodel\"\n\n\n… and pass the data list and the modified Stan code to rstan::stan() to fit the model.\n\n\nstanfit <- rstan::stan(\n  model_code = scode_car1,\n  data = sdata,\n  sample_file = \"carmodel\", # output in csv format\n  seed = stan_seed\n)\n\n\n\n\n\nOnce the chains have finished running, feed the Stan model back into brms, and have a look at the output of summary(). There are no obvious problems with convergence, given the lack of warnings from Stan and the \\(\\widehat{r}\\) values being very close to 1.\n\n\nfit <- brm(formula, data = subset, empty = TRUE)\nfit$fit <- stanfit\nfit <- rename_pars(fit)\nsummary(fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ ar(time = x, gr = g) \n   Data: subset (Number of observations: 120) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nar[1]     0.71      0.07     0.57     0.83 1.00     3547     2922\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.20      0.25    -0.67     0.30 1.00     3045     2351\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.17      0.08     1.03     1.33 1.00     3573     2603\n\nDraws were sampled using (). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nSince the brmsfit object doesn’t contain the CAR(1) formula, a few extra steps are needed to generate predictions. First, we generate draws from the model without the autocorrelation structure, then we apply a CAR(1) filter to the data, and then we summarize the filtered draws using median_qi(). Plot the medians along with the data, adding the 0.025 and 0.975 quantiles as a ribbon:\n\n\n# extract AR(1) draws:\nphi <- as_draws_df(fit, \"ar[1]\") %>% \n  as_tibble()\n\n# generate draws from the model without the autocorrelation structure:\npred_car1 <- tidybayes::add_epred_draws(subset, fit, incl_autocor = FALSE) %>% \n  ungroup() %>% \n  select(-c(.chain, .iteration)) %>% \n  arrange(.draw, g, x) %>% \n  left_join(phi, by = \".draw\") %>% \n  # add the CAR(1) structure:\n  group_by(.draw, g) %>% \n  mutate(\n    r_lag = replace_na(lag(y - .epred), 0),\n    .epred = .epred + r_lag * `ar[1]` ^ d_x\n  ) %>% \n  ungroup() %>% \n  # summarize:\n  select(-c(r_lag, `ar[1]`)) %>% \n  group_by(across(matches(paste(names(subset), collapse = \"|\")))) %>% \n  ggdist::median_qi() %>% \n  ungroup()\n\n\n\n\n\nAnd that’s it! The model recovers the parameters used to generate the simulated data well in this case—the mean of the posterior of \\(\\phi\\) is 0.71—which is a good sign that we’re on the right track.\n\n\n\nPinheiro, Jose, Douglas Bates, Saikat DebRoy, Deepayan Sarkar, and R Core Team. 2021. nlme: Linear and Nonlinear Mixed Effects Models. https://CRAN.R-project.org/package=nlme.\n\n\n\n\n",
    "preview": "posts/2022-02-01-building-a-continuous-time-autoregressive-model-in-brms-and-stan/building-a-continuous-time-autoregressive-model-in-brms-and-stan_files/figure-html5/plot-1.png",
    "last_modified": "2023-03-13T09:53:28-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-08-diagnostics-for-censored-autoregressive-models-fit-with-brms/",
    "title": "Diagnostics for censored autoregressive models fitted with brms",
    "description": "Posterior predictive checks and simulated residuals.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-10-08",
    "categories": [],
    "contents": "\n\n\n\nIn a previous post (A brownification pause), I made an attempt at tackling a common problem in environmental science: analyzing autocorrelated time series with left-censored values (i.e., nondetects). As I’ve learned, one powerful tool for this type of problem is brms (Bürkner 2017, 2018), an R package for fitting Bayesian regression models via Stan (Stan Development Team 2021).\nThere are, however, relatively few tools that I’m aware of for posterior predictive checks of censored regression models. The R function bayesplot::ppc_km_overlay() is one, but it is only suitable for right-censored data, which are less common in environmental time series.\nHere I use a similar approach to generate a posterior predictive check for a left-censored model. I use the R function NADA::cenfit() (Lee 2020) to estimate the empirical cumulative distribution function (ECDF) of the series and the posterior draws from the model. The function works by “flipping” the input—subtracting all values from a constant larger than any value—and estimating the ECDF according to the Kaplan-Meier method (for right-censored data).\n\n\n\n\n\n\nThe following generates ECDFs of the data and posterior predictions according to this method:\n\n\npp_ecdf <- function(model, newdata, yval, log_t = TRUE) {\n  \n  ecdf_data_in <- newdata %>% \n  # convert censoring indicator to logical:\n  mutate(ci = ci == -1) %>% \n  filter(!is.na({{yval}}))\n  \n  ecdf_data <- NADA::cenfit(\n    obs = if(log_t) {\n      log(pull(ecdf_data_in, {{yval}}))\n    } else {\n      pull(ecdf_data_in, {{yval}})\n    },\n    censored = ecdf_data_in$ci\n  )\n\necdf_pp <- tidybayes::add_predicted_draws(\n    newdata, \n    model,\n    ndraws = 200\n  ) %>% \n  ungroup() %>% \n  group_by(.draw) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    cenfit = map(data, \n      ~ with(.x, \n        NADA::cenfit(\n          obs = .prediction, \n          censored = rep(FALSE, length(.prediction))\n        )\n      )\n    ),\n    cenfit_summ = map(cenfit, summary)\n  ) %>% \n  unnest(cenfit_summ) %>% \n  select(where(~ !is.list(.x))) \n\n  bind_rows(\n    \"Posterior draws\" = ecdf_pp,\n    \"Observations\" = summary(ecdf_data),\n    .id = \"type\"\n  )\n}\n\n\n\nHere I’ve superimposed the ECDF of the time series on the ECDFs estimated using 200 draws from the posterior distribution of the brms::brm() model. From this plot, it appears that the posterior draws approximate the data reasonably well.\n\n\n\nAnother difficulty in evaluating models fitted to censored time series is residuals analysis. Here I adopt the approach of Wang and Chan (2018), generating simulated residuals by substituting censored and missing values of the time series with a draw from the posterior distribution of the fitted model and refitting the model on the augmented data. I then generated residual draws from the updated model.\nThe function below does the simulation:\n\n\nsimulate_residuals <- function(\n  model, \n  newdata, \n  yval, \n  file, \n  seed = NULL,\n  ...\n) {\n  \n  data_aug <- tidybayes::add_predicted_draws(\n    newdata,\n    model,\n    seed = seed,\n    ndraws = 1\n  ) %>% \n    ungroup() %>% \n    mutate(\n      value = if_else(\n        is.na({{yval}}) | ci == -1,\n        .prediction,\n        {{yval}}\n      ),\n      ci = 0 # no censoring\n    ) %>%\n    select(-starts_with(\".\"))\n  \n  model_update <- update(\n    model, \n    newdata = data_aug,\n    file = file,\n    cores = 4,\n    seed = seed,\n    ...\n  )\n  \n  model_resids <- tidybayes::add_residual_draws(\n    object = model_update,\n    newdata = data_aug,\n    method = \"posterior_epred\"\n  )\n  \n  list(\n    model = model_update, \n    residuals = model_resids, \n    data = data_aug\n  )\n  \n}\n\n\n\n\n\n\nHere is the density of the lag one autocorrelation, estimated using residual draws from Bayesian GAMs fitted with and without a first-order autoregressive (AR(1)) term. There is some indication here that the GAM with an AR(1) term is accounting for residual autocorrelation.\n\n\n\nFor a bit more verification, I fitted a similar GAM to a simulated dataset, generated by adding an AR(1) series to a nonlinear trend, as follows:\n\n\nlcl <- 10 # lower censoring limit\n\nsimdat <- withr::with_seed(101, {\n  tibble(\n  x = 1:200,\n  y_t = 1e-3 * x + 1e-4 * x ^ 2,\n  e = arima.sim(list(ar = .5), length(y_t)) %>% \n    as.numeric(),\n  y = y_t + e + 10,\n  y_star = pmax(y, lcl),\n  ci = if_else(y < lcl, -1, 0)\n)\n})\n\n\n\nAgain, I fitted the GAM—and the equivalent model without an AR(1) term—using brms:\n\n\nmodel_simdat <- brm(\n  bf(y_star | cens(ci) ~ s(x) + ar(time = x, p = 1)),\n  data = simdat,\n  seed = 124,\n  # save the model:\n  file = here::here(\"model_simdat\"),\n  cores = 4,\n  control = list(adapt_delta = .99)\n)\n\n\n\nHere are the simulated data and the fitted model:\n\n\n\n\n\n\nAnd here is the ECDF overlay described above:\n\n\n\n\n\n\nThe first-order autocorrelation estimate from the simulated residuals suggests that the model is accounting for autocorrelation in the residuals, as expected:\n\n\n\nAnd the estimate of the AR(1) term is 0.57, which is similar to true value of 0.5.\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nLee, Lopaka. 2020. NADA: Nondetects and Data Analysis for Environmental Data. https://CRAN.R-project.org/package=NADA.\n\n\nStan Development Team. 2021. “RStan: The R Interface to Stan.” https://mc-stan.org/.\n\n\nWang, Chao, and Kung-Sik Chan. 2018. “Quasi-Likelihood Estimation of a Censored Autoregressive Model With Exogenous Variables.” Journal of the American Statistical Association 113 (523): 1135–45. https://doi.org/10.1080/01621459.2017.1307115.\n\n\n\n\n",
    "preview": "posts/2021-10-08-diagnostics-for-censored-autoregressive-models-fit-with-brms/diagnostics-for-censored-autoregressive-models-fit-with-brms_files/figure-html5/ecdf-sim-1.png",
    "last_modified": "2022-02-11T09:03:02-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-08-a-brownification-pause/",
    "title": "A brownification pause?",
    "description": "Censored autoregression with a smoothed time covariate.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-09-08",
    "categories": [],
    "contents": "\n\n\n\nI recently came across a new paper titled “Brownification on hold: What traditional analyses miss in extended surface water records” (Eklöf et al. 2021). In it, the authors argue that linear trend analysis misses important nonlinearities in surface water quality records. Specifically, the paper suggests that surface water browning—increases in coloured organic matter—has paused in recent decades and that generalized additive modeling would be better suited than linear regression to describe that pause.\nEarlier this year I cowrote an article (Redden et al. 2021) describing a linear trend analysis of surface water time series in Nova Scotia. We found strong evidence of browning in many surface waters, but I thought I’d briefly revisit that analysis here to see if we missed anything that a nonlinear trend analysis wouldn’t. This isn’t meant to be a full reanalysis of the data, but it might point in the direction that a future analysis would take.\nThere is a significant roadblock to fitting generalized additive models (GAMs) to the data from our paper: GAMs—at least, as they are implemented in the popular R package mgcv—do not allow censored autoregression (models of autocorrelated time series where part of the series is censored).\nOne option for censored autoregression is the carx package in R. It’s primary function, carx::carx(), also permits a matrix of external covariates that can include a basis expansion—making it possible to fit a GAM to a left-censored, autocorrelated time series.\nIn this post I demonstrate the package on a single measurement series, one of many we used in Redden et al. (2021) The data are sourced from Environment Canada, as described in the paper.\n\n\n\n\n\n\nIn the series, 10% of apparent colour values—those below 5 Pt-Co—are left-censored. To generate a regularly spaced time series, I aggregated the data into semesters by taking medians, and I excluded values collected before 1983. When aggregation required taking the midpoint of a left-censored and an observed value, the output was left-censored at the midpoint between the observed value and the censoring limit.\nI chose to handle missing values by left-censoring them at a limit of positive infinity, as implemented in carx(). The response was log-transformed prior to fitting the model, and the matrix of covariates comprised a cubic regression spline basis expansion of the time variable using splines::bs() with 4 degrees of freedom. The model also included semester as a binary covariate to account for seasonal variation.\n\n\n\nAs a point of comparison, I fit a separate censored autoregression with a linear time covariate. Here are the linear model and GAM predictions along with 95% confidence intervals on the predicted values:\n\n\n\nWhile the GAM does appear to track the series slightly better, the linear model yielded an AIC of -44, whereas the cubic regression spline model yielded a larger AIC of -39.\nBoth models yielded residuals with no obvious deviations from whiteness. For comparison, equivalent models fit using the function lm() (no autoregression) are shown as well. Autocorrelation at lag 1 is notably lower in the residuals from the censored AR(1) models.\n\n\n\nBoth autoregressions yielded residuals that were approximately Gaussian, albeit with somewhat fatter tails than expected (the carx method is robust against mild departures from normality (Wang and Chan 2017)).\n\n\n\nThe variance of the residuals was also reasonably constant, apart from a few outliers and perhaps a slight reduction in later years.\n\n\n\nBut overall, there’s not much evidence here that the GAM describes this particular series any better than the linear model. And a GAM fit using brms (“Bayesian Regression Models using Stan”)—which accomodates both left-censoring and autoregression—yielded similar results. Here is a sketch of the model one might fit using brms::brm(). N.B., missing values are handled differently here: see this vignette.\nWhile I haven’t fully evaluated the fit of this model, it doesn’t suffer from any major issues in terms of convergence. And it is worth noting that it yields a smooth curve that differs little from the censored autogregression with linear time covariate. Fitted values from the latter model—including the AR(1) component—are superimposed in red on the following plot.\n\n\nmodel_in_brms <- model_in %>% \n  mutate(\n    # replace missing values in the censoring indicator with 0 (uncensored)\n    ci = replace_na(ci, 0)\n  )\n\nmodel_brms <- brm(\n  bf(log(value) | cens(ci) + mi() ~\n       s(numeric_date) + sem + ar(time = numeric_date, p = 1)),\n  data = model_in_brms,\n  control = list(adapt_delta = .999),\n  seed = 3152,\n  file = \"model_brms\"\n)\n\n\n\n\n\n\nThe brms and carx models also yielded similar estimates of the AR(1) parameter: 0.21 for the censored autoregression (linear time covariate) and 0.28 for the Bayesian regression model. In percentage terms, the two approaches differ mainly in the predictions over the first part of the series, where most of the censored observations were recorded.\n\n\n\nEklöf, Karin, Claudia von Brömssen, Nino Amvrosiadi, Jens Fölster, Marcus B. Wallin, and Kevin Bishop. 2021. “Brownification on Hold: What Traditional Analyses Miss in Extended Surface Water Records.” Water Research 203 (September): 117544. https://doi.org/10.1016/j.watres.2021.117544.\n\n\nRedden, David J, Benjamin F Trueman, Dewey W Dunnington, Lindsay E Anderson, and Graham Gagnon. 2021. “Chemical Recovery and Browning of Nova Scotia Surface Waters in Response to Declining Acid Deposition.” Environmental Science: Processes & Impacts, 10.1039.D0EM00425A. https://doi.org/10.1039/D0EM00425A.\n\n\nWang, Chao, and Kung-Sik Chan. 2017. “Carx: An R Package to Estimate Censored Autoregressive Time Series with Exogenous Covariates.” The R Journal 9 (2): 213. https://doi.org/10.32614/RJ-2017-064.\n\n\n\n\n",
    "preview": "posts/2021-09-08-a-brownification-pause/a-brownification-pause_files/figure-html5/plot-carx-1.png",
    "last_modified": "2021-12-27T15:28:48-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 864
  },
  {
    "path": "posts/2021-08-13-new-preprint/",
    "title": "New preprint!",
    "description": "Aluminum in drinking water can interact with orthophosphate, increasing lead solubility.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [],
    "contents": "\nRecently, my coauthors and I used a preprint server (chemRxiv) to share an early draft of a research paper for the first time. Preprints are not very common in my field, but I imagine that will change in the future as their advantages become clear. The paper, “Seasonal lead release to drinking water and the effect of aluminum” (Trueman et al. 2021), explores the role of aluminum in limiting the availability of orthophosphate for lead corrosion control in drinking water systems.\nOrthophosphate works by forming an insoluble precipitate with lead, but precipitation with other metals can limit its effect on lead solubility. Here, I reproduce some of the code included in the paper to account for aluminum phosphate precipitation in calculating equilibrium lead solubility. Solubility modeling is done here using pbcusol, an interface for PHREEQC in R that is geared specifically to lead and copper solubility predictions. I also extend the analysis to include other metals that might precipitate with orthophosphate and interfere with lead phosphate formation.\nThe concentrations of interfering aluminum, calcium, manganese, and iron have been chosen to approximate typical ranges, although more extreme values are certainly possible or even likely. A caveat: the phases that precipitate in the model and limit available orthophosphate—variscite (AlPO4·2H2O), vivianite (Fe3(PO4)2·8H2O, CaHPO4, and MnHPO4—may not be the phases that precipitate in drinking water systems.\nTo reproduce the model, load the necessary packages and define a few chemical equations that are not included in the default (pbcusol) database.\n\n\nlibrary(\"tidyverse\")\n# remotes::install_github(\"bentrueman/pbcusol)\nlibrary(\"pbcusol\") \n\nsolids <- list(\n  \"Variscite\",\n  \"AlPO4:2H2O = Al+3 + PO4-3 + 2H2O\",\n  \"log_k\" = -22.36, # https://doi.org/10.1016/j.gca.2010.10.012\n  \"CaHPO4\",\n  \"CaHPO4 = Ca+2 + H+ + PO4-3\",\n  \"log_k\" = -19.275 # from phreeqc::minteq.v4.dat\n)\n\naqueous_species <- list( \n  # from https://doi.org/10.1016/j.gca.2010.10.012 and \n  # https://doi.org/10.1080/09593332708618735\n  \"HPO4-2 + Al+3 = AlHPO4+\",\n  \"log_k\" = 7.4,\n  \"HPO4-2 + H+ + Al+3 = AlH2PO4+2\",\n  \"log_k\" = 3.1\n)\n\n\n\nThen, define a couple of functions that we will use to build the model. The first is a wrapper around pbcusol::eq_sol_fixed() that saves having to code the same arguments repeatedly. The second builds a list of the arguments to pbcusol::eq_sol_fixed() that change across iterations.\n\n\npb_sol_custom <- function(interference = \"Variscite\", ...) {\n      pbcusol::eq_sol_fixed(element = \"Pb\",\n        ph = 7.3, dic = 5,  \n        Na = 10 / chemr::mass(\"Na\"),\n        eq_phase_components = rlang::list2(!!interference := c(0, 0)),\n        new_species = aqueous_species,\n        new_phase = solids,\n        ...\n    ) %>% \n    select(pb_ppb)\n}\n\nbuild_args <- function(x) {\n  list(x$metal_conc / chemr::mass(x$metal)) %>% \n    set_names(nm = x$metal) %>% \n    c(list(\n      phase = x$phase,\n      interference = x$interference,\n      phosphate = x$po4\n    ))\n}\n\n\n\nLead solubility is calculated over a grid of input values in parallel, using future::plan().\n\n\ngrid_size <- 10\n\nfuture::plan(\"multisession\") # parallel iteration\n\nout <- crossing(\n  phase = c(\"Hxypyromorphite\", \"Cerussite\"),\n  po4 = seq(0.1, 2, length.out = grid_size),\n  metal = c(\"Ca\", \"Al\", \"Mn\", \"Fe\"), \n  metal_conc = seq(0, .5, length.out = grid_size)\n) %>% \n  mutate(\n    interference = fct_recode(metal, \n      \"Variscite\" = \"Al\", \"MnHPO4(C)\" = \"Mn\", \n      \"Vivianite\" = \"Fe\", \"CaHPO4\" = \"Ca\"\n    ) %>% \n      as.character(),\n    # adjust metal concentrations to approximate typical values:\n    metal_conc = case_when(\n      metal == \"Ca\" ~ metal_conc * 200,\n      metal == \"Fe\" ~ metal_conc * 2,\n      metal == \"Mn\" ~ metal_conc * .5,\n      TRUE ~ metal_conc\n    )\n  ) %>% \n  rowid_to_column() %>% \n  group_by(rowid) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    args = map(data, build_args),\n    pb_ppb = furrr::future_map(args, ~ do.call(pb_sol_custom, .x))\n  )\n\n\n\nAnd here are the results. At pH 7.3 and a dissolved inorganic carbon concentration of 5 mg C L-1, all four metals impact lead solubility to some extent, and the predicted effect of manganese is largest.\n\n\n\n\n\n\nTrueman, Benjamin F, Aaron Bleasdale-Pollowy, Javier A Locsin, Jessica L Bennett, Wendy H Krkošek, and Graham A Gagnon. 2021. “Seasonal Lead Release to Drinking Water and the Effect of Aluminum.” Preprint. Chemistry. https://doi.org/10.33774/chemrxiv-2021-59nd6-v2.\n\n\n\n\n",
    "preview": "posts/2021-08-13-new-preprint/new-preprint_files/figure-html5/plot-1.png",
    "last_modified": "2021-08-16T16:21:53-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/",
    "title": "Comparing water quality time series using a generalized additive mixed model",
    "description": "Revisiting work from 2016 to better model time series with non-linear trends.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-07",
    "categories": [],
    "contents": "\nIn a 2016 paper (Trueman and Gagnon 2016), I evaluated the effect of cast iron distribution mains on the lead concentrations due to lead pipes downstream from those mains. This question has relevance for minimizing lead in drinking water and for prioritizing lead pipe replacement; if a lead pipe is connected to an unlined iron distribution main, lead levels reaching the consumer are likely to be higher.\nIn the paper, I used the arima() function in R with a matrix of external regressors to account for the effect of the distribution main and autocorrelation in the time series of lead concentrations. But linear regression was only a rough approximation of the concentration time series’ behaviour, and I think using a generalized additive model would have been a better choice. Here, I revisit those data, using mgcv::gamm() to fit a generalized additive mixed model and nlme::corCAR1() to include a continuous time first-order autoregressive error structure.\nFirst, I built the model, using s() to fit a separate smooth to each category of lead time series. (The categories are defined by the distribution main—PVC or cast iron—and the lead pipe configuration—full lead or half lead, half copper.) In this model, the smooths differ in their flexibility and shape (Pedersen et al. 2019). They are centered, so the grouping variables are added as main effects (see the documentation for mgcv::s()). I use tidyr::nest() to allow for list columns that include the model and predicted values along with the data.\n\n\nfe_gam <- jdk_pl %>% \n  filter(fraction == \"total\") %>% \n  mutate_if(is.character, factor) %>% \n  mutate(lsl_grp = interaction(pipe_config, main)) %>% \n  arrange(fraction, lsl, time_d) %>% \n  group_by(fraction) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    model = map(\n      data,\n      ~ mgcv::gamm(\n        log(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config, # model I\n        correlation = nlme::corCAR1(form = ~ time_d | lsl),\n        method = \"REML\",\n        data = .x\n      )\n    )\n  )\n\n\n\nNext, I predicted from the model over the range of x values, and constructed a pointwise (approximate) 95% confidence band using the standard errors of the fitted values.\n\n\nfe_gam <- fe_gam %>% \n  mutate(\n    preds = map2(\n      model, data, \n      ~ predict(.x$gam, newdata = .y, se = TRUE)\n    ),\n    preds = map2(\n      preds, model, \n      ~ tibble(fit = .x$fit, se_fit = .x$se.fit) %>% \n        mutate(\n          lwr = fit - 2 * se_fit,\n          upr = fit + 2 * se_fit,\n          fit = fit\n        ) %>% \n        mutate_at(vars(c(fit, lwr, upr)), exp)\n    )\n  )\n\n\n\nHere are the data, the fitted model, and the confidence bands:\n\n\n\nYou’ll notice that some of the smooths—especially the Full LSL/PVC smooth—are smoother than the eye would expect. This is probably because the model is attributing some of the nonlinearity to autocorrelation, something discussed in more detail elsewhere (Simpson 2018).\nThe model does a reasonably good job—but not a perfect job—accounting for autocorrelation in the time series. “Raw” and “normalized” residuals are defined in the help file to nlme::residuals.lme() under type. Essentially, raw residuals represent the difference between the observed and fitted values, while normalized residuals account for the estimated error structure. The grey shaded band in the figure below represents a 95% confidence interval on the autocorrelation of white Gaussian noise.\n\n\n\nThe natural log transformation yields a model with residuals that are approximately normal.\n\n\n\nFinally, let’s have a look at the model summary. The effect of the distribution main is statistically significant, as is the effect of pipe configuration (which we’re less concerned about here). Based on the retransformed coefficient (exponentiating and subtracting one), the model estimates that lead release is 51% lower when the distribution main supplying the lead pipe is plastic as opposed to iron—an important result given the limited resources available to replace lead drinking water pipes.\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config\n\nParametric coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.44408    0.07202  75.587   <2e-16 ***\nmainpvc                -0.70833    0.08333  -8.501   <2e-16 ***\npipe_configpartial lsl -0.84626    0.08333 -10.156   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                    edf Ref.df      F p-value    \ns(time_d):lsl_grpfull lsl.iron    1.000  1.000  7.718 0.00558 ** \ns(time_d):lsl_grppartial lsl.iron 1.000  1.000  9.904 0.00170 ** \ns(time_d):lsl_grpfull lsl.pvc     1.000  1.000 22.649   3e-06 ***\ns(time_d):lsl_grppartial lsl.pvc  3.444  3.444  3.345 0.00880 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.564   \n  Scale est. = 0.33246   n = 954\n\n\n\n\nPedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross. 2019. “Hierarchical Generalized Additive Models in Ecology: An Introduction with Mgcv.” PeerJ 7 (May): e6876. https://doi.org/10.7717/peerj.6876.\n\n\nSimpson, Gavin L. 2018. “Modelling Palaeoecological Time Series Using Generalised Additive Models.” Frontiers in Ecology and Evolution 6 (October): 149. https://doi.org/10.3389/fevo.2018.00149.\n\n\nTrueman, Benjamin F., and Graham A. Gagnon. 2016. “Understanding the Role of Particulate Iron in Lead Release to Drinking Water.” Environmental Science & Technology 50 (17): 9053–60. https://doi.org/10.1021/acs.est.6b01153.\n\n\n\n\n",
    "preview": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models_files/figure-html5/plot-1.png",
    "last_modified": "2021-08-04T14:57:00-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 672
  },
  {
    "path": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/",
    "title": "Non-parametric matched pair testing with left-censored data",
    "description": "Comparing two groups of measurements when some values are below one or multiple detection limit(s).",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nThere are relatively few options in R for comparing matched pairs in two groups with left-censored data. And while NADA2::cen_signedranktest() is an excellent tool, I wrote the following function as another. It implements the Paired Prentice-Wilcoxon test, as described in Helsel (2012). I should also acknowledge the USGS’ orphaned package smwrQW (Lorenz 2017) for its version of the same test.\n\n\nlibrary(\"tidyverse\")\nlibrary(\"survival\")\n\nppw_test <- function(\n  x, # numeric\n  y, # numeric\n  x_cen, # logical \n  y_cen, # logical \n  alternative = \"two.sided\", # either \"two.sided\", \"greater\" (x > y), or \"less\" (x < y)\n  flip = TRUE\n) {\n  \n  if(length(x) != length(y))\n    stop(\"Lengths of x and y must be the same for paired data.\")\n  \n  if(any(c(x, y) < 0))\n    stop(\"Negative values in x or y.\")\n  \n  valid_alternative <- pmatch(alternative, c(\"two.sided\", \"greater\", \"less\"))\n  \n  if(is.na(valid_alternative))\n    stop('Invalid choice for alternative, must match \"two.sided\", \"greater\", or \"less\"')\n\n  test_input <- tibble(x_val = x, y_val = y, x_cen, y_cen) %>% \n    na.omit() %>% \n    rowid_to_column() %>% \n    pivot_longer(\n      cols = -rowid,\n      names_to = c(\"group\", \".value\"),\n      names_pattern = \"(.+)_(.+)\"\n    ) %>% \n    mutate(\n      cen = as.numeric(!cen), # 0 is censored, 1 is observed\n      # flip data so that smallest observation becomes longest \"survival time\":\n      # N.B., this rounds the flipped data to 6 decimal places\n      flipped = if(flip) {max(val) + 1 - val} else val,\n      flipped = round(flipped, 6)\n    ) %>% \n    left_join(\n      # estimate survival function:\n      survival::survfit(survival::Surv(flipped, cen) ~ 1, data = .) %>% \n        broom::tidy() %>% \n        mutate(time = round(time, 6)),\n      by = c(\"flipped\" = \"time\")\n    ) %>% \n    mutate(score = if_else(cen == 1, 1 - 2 * estimate, 1 - estimate)) %>% \n    pivot_wider(id_cols = rowid, names_from = group, values_from = score) %>% \n    mutate(d = x - y) %>% \n    summarize(\n      z_ppw = sum(d) / sqrt(sum(d ^ 2)),\n      p_val = if(alternative == \"two.sided\") {2 * pnorm(abs(z_ppw), lower.tail = FALSE)} else\n        if(alternative == \"greater\") {pnorm(-z_ppw, lower.tail = FALSE)} else\n          if(alternative == \"less\") {pnorm(z_ppw, lower.tail = FALSE)} else\n            \"alternative hypothesis is invalid\"\n    )\n  \n  list(\"statistic\" = test_input$z_ppw, \"p.value\" = test_input$p_val)\n  \n}\n\n\n\n\n\n\nThe ppw_test() function works like this:\n\n\nwithr::with_seed(450, { # generate two random normal variables, with left-censoring:\n  tibble( \n    x = rnorm(10, 3, 1),\n    y = rnorm(10, 3, 1),\n    x_cen = x < 2,\n    y_cen = y < 2\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen))\n\n\n$statistic\n[1] -0.5628925\n\n$p.value\n[1] 0.5735081\n\nIt also does one-sided tests:\n\n\nwithr::with_seed(23, {\n  tibble( \n    x = rnorm(10, 10, 1),\n    y = rnorm(10, 5, 1),\n    x_cen = x < 10,\n    y_cen = y < 5\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen, alternative = \"greater\")) \n\n\n$statistic\n[1] -2.918231\n\n$p.value\n[1] 0.001760118\n\nThe following code tests that ppw_test() gives the expected result when applied to the data in Table 9.7 of Helsel (2012). First, here are the data:\n\n\n\n\n\n\nAnd here are the test results. These are close to the values in Helsel (2012), but not exactly the same. I suspect there are a few typos in the table, which may have something to do with it. For example, while most of the scores calculated by ppw_test() are consistent with those reported in the table, line 11, column 3 list a score of 0.55 for the second-highest value in column 1, while ppw_test() calculates a score of -0.54, much closer to the value of -0.67 corresponding to the largest value in column 1. There is a similar problem on line 12.\n\n\nhelsel %>% \n  mutate(\n    june_cen = str_detect(june, \"<\"),\n    sept_cen = str_detect(september, \"<\"),\n  ) %>% \n  mutate_if(is.character, ~ as.numeric(str_remove(.x, \"<\"))) %>% \n  with(ppw_test(june, september, june_cen, sept_cen, \"less\"))\n\n\n$statistic\n[1] 3.012394\n\n$p.value\n[1] 0.001295979\n\n\n\n\n\n\n\nHelsel, Dennis R. 2012. Statistics for Censored Environmental Data Using Minitab and R. 2nd ed. Wiley Series in Statistics in Practice. Hoboken, N.J: Wiley.\n\n\nLorenz, Dave. 2017. “SmwrQW–an R Package for Managing and Analyzing Water-Quality Data, Version 0.7.9.” Open File Report. U.S. Geological Survey.\n\n\n\n\n",
    "preview": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/non-parametric-matched-pair-testing-with-left-censored-data_files/figure-html5/helsel-plot-1.png",
    "last_modified": "2021-07-28T17:16:05-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 576
  },
  {
    "path": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/",
    "title": "Peak detection for qualitative XRD analysis in R",
    "description": "An improved workflow for visualizing X-ray diffraction data in R.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-29",
    "categories": [],
    "contents": "\n\n\n\nA important aspect of understanding inorganic contaminant mobility in drinking water systems is identifying the solid phases that exist on the interior surfaces of pipes. This is frequently done with X-ray diffraction (XRD), and I am often preparing figures to communicate XRD data. This post provides an example workflow for displaying XRD data and automatically labelling the peaks so that readers can easily attribute them to the appropriate solid phases. It makes use of the peak detection function peak_maxima() in my field-flow fractionation data analysis package fffprocessr.\nFor this example, we’ll use an XRD pattern representing a mixed lead carbonate phase containing cerussite (PbCO3) and hydrocerussite (Pb3(CO3)2(OH)2). Here are the data, along with a couple of quick plotting functions to avoid repetition later on:\n\n\nlabel_axes <- function(...) {\n  labs(\n    x = expression(\"2\"*theta~\"(Cu K\"*alpha*\")\"),\n    y = \"Intensity\",\n    ...\n  )\n}\n\ncustom_colour_scale <- function(n = 3, ...) {\n  scale_colour_manual(values = wesanderson::wes_palette(\"Zissou1\", n)[c(1, n)])\n}\n\ndata %>%\n  ggplot(aes(two_theta, intensity)) +\n  geom_line() +\n  label_axes()\n\n\n\n\nFirst we need a quick and dirty method of determining the order in which phases should appear in the plot—interpretation is easiest when the standard that matches the data best appears closest to it.\n\n\nxrd_corr <- function(run, standard) {\n  list( \n      sample = run, \n      std = standard\n    ) %>% \n    bind_rows(.id = \"type\") %>% \n    pivot_wider(id_cols = two_theta, names_from = type, values_from = intensity) %>% \n    group_by(two_theta = round(two_theta)) %>% \n    summarize(sample = median(sample, na.rm = TRUE), std = median(std, na.rm = TRUE)) %>% \n    ungroup() %>% \n    with(cor(sample, std, use = \"complete\", method = \"pearson\"))\n}\n\nimportance <- stds %>% \n  distinct(phase) %>% \n  pull(phase) %>% \n  set_names() %>% \n  map_dfc(\n    ~ xrd_corr(\n        run = data,\n        standard = filter(stds, phase == .x) %>% distinct()\n    )\n  ) %>% \n  pivot_longer(everything(), names_to = \"phase\", values_to = \"r\") %>% \n  arrange(desc(r))\n\n\n\nHere are the data with the standard patterns for cerussite and hydrrocerussite. Cerussite is a better match to the data and so it is plotted closest to the data.\n\n\nordered_stds <- stds %>% \n  mutate(phase_f = factor(phase) %>% fct_relevel(importance$phase) %>% as.numeric())\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nWe detect peaks in the pattern using fffprocessr::peak_maxima().\n\n\npeaks_detected <- data %>% \n  peak_maxima(\n    peaks = 23, n = 1, method = \"sigma\", \n     x_var = \"two_theta\", y_var = \"intensity\",\n    group_vars = NULL\n  )\n\n\n\nThen, we need a function to assign the detected peaks to the appropriate standard.\n\n\nassign_peaks <- function(sample, standard, tol = 1, phases) { \n  sample %>% \n    mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n    right_join(\n      standard %>% \n        filter(phase %in% phases) %>% \n        mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n        select(two_theta, two_theta_rnd, phase), \n      by = \"two_theta_rnd\", suffix = c(\"\", \"_std\")\n    ) %>% \n    group_by(two_theta = round(two_theta_std, 1), phase) %>%\n    summarize(intensity = max(intensity)) %>%\n    ungroup()\n}\n\npeaks_idd <- bind_rows(\n  assign_peaks(peaks_detected, stds, phases = importance$phase[1], tol = .5),\n  assign_peaks(peaks_detected, stds, phases = importance$phase[2], tol = .5)\n)\n\n\n\nFinally, we add the identified peaks to the plot:\n\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  geom_point(\n    data = peaks_idd,\n    aes(col = phase)\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nAnd there we have it—the XRD data with and the appropriate standard patterns, with each peak in the data labelled according to the corresponding peak in one of the standards. Of course, this is a relatively crystalline sample, and the signal-to-noise ratio is high; in a future post I’ll test this workflow out on some noisy XRD data.\n\n\n\n",
    "preview": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/peak-detection-for-qualitative-xrd-analysis-in-r_files/figure-html5/data-w-peaks-1.png",
    "last_modified": "2021-10-22T10:04:53-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 672
  },
  {
    "path": "posts/2021-06-18-lead-solubility-prediction-using-shiny/",
    "title": "Lead solubility prediction using R and Shiny",
    "description": "An R package and Shiny app for PHREEQC-based equilibrium lead and copper solubility prediction.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-18",
    "categories": [],
    "contents": "\nRecently, I’ve been experimenting with the excellent R package tidyphreeqc (Dunnington 2019), which provides a convenient interface for generating PHREEQC input files (Charlton and Parkhurst 2011; Parkhurst and Appelo 2013). tidyphreeqc incorporates easily into my workflow, and it’s found its way into several recent publications (Li et al. 2021, 2020).\nI’ve also used it to build a separate package, pbcusol, that handles lead and copper solubility prediction specifically, with curated thermodynamic data and some domain specific features implemented in what is hopefully a straightforward collection of functions. pbcusol is implemented as a Shiny app, available here.\npbcusol can be used to generate lead and copper solubility predictions that are comparable with those found in literature (Schock, Wagner, and Oliphant 1996), as detailed in the package README on GitHub. (They’re reproduced here using the wesanderson package for the colour palette (Ram and Wickham 2018).)\n\n\n\n\n\n\npbcusol can also be used to generate predicted lead and copper solubility in the presence of humic substances. This is also outlined in the package README, but here is an example of the type of visualization that can be generated, along with the code necessary to reproduce the model output. Of course, these predictions should be properly validated—more on that later.\n\n\ngrid_dim <- 25 # dimension of solubility grid square\n\nfuture::plan(\"multisession\")\n\nnom_grid <- crossing(\n  pH_in = seq(6.5, 10.5, length.out = grid_dim),\n  nom_in = seq(0, 3e-3, length.out = grid_dim)\n) %>% \n  rowid_to_column() %>% \n  group_by(rowid) %>% \n  nest() %>% \n  ungroup() %>%  \n  mutate(\n    model = furrr::future_map(data, \n      ~ with(.x, \n        pb_sol_wham(\n          ph = pH_in, dic = 5, phase = \"Hydcerussite\", \n          Na = 10 / chemr::mass(\"Na\"), mass_ha = nom_in\n        )\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n\nCharlton, S. R., and D. L. Parkhurst. 2011. “Modules Based on the Geochemical Model Phreeqc for Use in Scripting and Programming Languages.” Computers & Geosciences 37: 1653–63. http://dx.doi.org/10.1016/j.cageo.2011.02.005.\n\n\nDunnington, Dewey. 2019. tidyphreeqc: Tidy Geochemical Modeling Using Phreeqc. https://github.com/paleolimbot/tidyphreeqc.\n\n\nLi, Bofu, Benjamin F. Trueman, Javier M. Locsin, Yaohuan Gao, Mohammad Shahedur Rahman, Yuri Park, and Graham A. Gagnon. 2021. “Impact of Sodium Silicate on Lead Release from Lead(II) Carbonate.” Environmental Science: Water Research & Technology 7 (3): 599–609. https://doi.org/10.1039/D0EW00886A.\n\n\nLi, Bofu, Benjamin F. Trueman, Mohammad Shahedur Rahman, and Graham A. Gagnon. 2020. “Controlling Lead Release Due to Uniform and Galvanic Corrosion—an Evaluation of Silicate-Based Inhibitors.” Journal of Hazardous Materials, December, 124707. https://doi.org/10.1016/j.jhazmat.2020.124707.\n\n\nParkhurst, D. L., and C. A. J. Appelo. 2013. Description of Input and Examples for Phreeqc Version 3–a Computer Program for Speciation, Batch-Reaction, One-Dimensional Transport, and Inverse Geochemical Calculations. Vol. book 6. Techniques and Methods. U.S. Geological Survey. https://pubs.usgs.gov/tm/06/a43.\n\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson Palette Generator. https://CRAN.R-project.org/package=wesanderson.\n\n\nSchock, M. R., I. Wagner, and R. J. Oliphant. 1996. “Corrosion and solubility of lead in drinking water.” In Internal corrosion of water distribution systems, 2nd ed., 131–230. Denver, CO: American Water Works Association Research Foundation.\n\n\n\n\n",
    "preview": "posts/2021-06-18-lead-solubility-prediction-using-shiny/lead-solubility-prediction-using-shiny_files/figure-html5/basic-grid-plot-1.png",
    "last_modified": "2021-08-16T15:55:42-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 576
  }
]
