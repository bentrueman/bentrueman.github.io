[
  {
    "path": "posts/2021-10-08-diagnostics-for-censored-autoregressive-models-fit-with-brms/",
    "title": "Diagnostics for censored autoregressive models fitted with brms",
    "description": "Posterior predictive checks and simulated residuals.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-10-08",
    "categories": [],
    "contents": "\n\n\n\nIn a previous post (A brownification pause), I made an attempt at tackling a common problem in environmental science: analyzing autocorrelated time series with left-censored values (i.e., nondetects). As I’ve learned, one powerful tool for this type of problem is brms (Bürkner 2017, 2018), an R package for fitting Bayesian regression models via Stan (Stan Development Team 2021).\nThere are, however, relatively few tools that I’m aware of for posterior predictive checks of censored regression models. The R function bayesplot::ppc_km_overlay() is one, but it is only suitable for right-censored data, which are less common in environmental time series.\nHere I use a similar approach to generate a posterior predictive check for a left-censored model. I use the R function NADA::cenfit() (Lee 2020) to estimate the empirical cumulative distribution function (ECDF) of the series and the posterior draws from the model. The function works by “flipping” the input—subtracting all values from a constant larger than any value—and estimating the ECDF according to the Kaplan-Meier method (for right-censored data).\n\n\n\n\n\n\nThe following generates ECDFs of the data and posterior predictions according to this method:\n\n\npp_ecdf <- function(model, newdata, yval, log_t = TRUE) {\n  \n  ecdf_data_in <- newdata %>% \n  # convert censoring indicator to logical:\n  mutate(ci = ci == -1) %>% \n  filter(!is.na({{yval}}))\n  \n  ecdf_data <- NADA::cenfit(\n    obs = if(log_t) {\n      log(pull(ecdf_data_in, {{yval}}))\n    } else {\n      pull(ecdf_data_in, {{yval}})\n    },\n    censored = ecdf_data_in$ci\n  )\n\necdf_pp <- tidybayes::add_predicted_draws(\n    newdata, \n    model,\n    ndraws = 200\n  ) %>% \n  ungroup() %>% \n  group_by(.draw) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    cenfit = map(data, \n      ~ with(.x, \n        NADA::cenfit(\n          obs = .prediction, \n          censored = rep(FALSE, length(.prediction))\n        )\n      )\n    ),\n    cenfit_summ = map(cenfit, summary)\n  ) %>% \n  unnest(cenfit_summ) %>% \n  select(where(~ !is.list(.x))) \n\n  bind_rows(\n    \"Posterior draws\" = ecdf_pp,\n    \"Observations\" = summary(ecdf_data),\n    .id = \"type\"\n  )\n}\n\n\n\nHere I’ve superimposed the ECDF of the time series on the ECDFs estimated using 200 draws from the posterior distribution of the brms::brm() model. From this plot, it appears that the posterior draws approximate the data reasonably well.\n\n\n\nAnother difficulty in evaluating models fitted to censored time series is residuals analysis. Here I adopt the approach of Wang and Chan (2018), generating simulated residuals by substituting censored and missing values of the time series with a draw from the posterior distribution of the fitted model and refitting the model on the augmented data. I then generated residual draws from the updated model.\nThe function below does the simulation:\n\n\nsimulate_residuals <- function(\n  model, \n  newdata, \n  yval, \n  file, \n  seed = NULL,\n  ...\n) {\n  \n  data_aug <- tidybayes::add_predicted_draws(\n    newdata,\n    model,\n    seed = seed,\n    ndraws = 1\n  ) %>% \n    ungroup() %>% \n    mutate(\n      value = if_else(\n        is.na({{yval}}) | ci == -1,\n        .prediction,\n        {{yval}}\n      ),\n      ci = 0 # no censoring\n    ) %>%\n    select(-starts_with(\".\"))\n  \n  model_update <- update(\n    model, \n    newdata = data_aug,\n    file = file,\n    cores = 4,\n    seed = seed,\n    ...\n  )\n  \n  model_resids <- tidybayes::add_residual_draws(\n    object = model_update,\n    newdata = data_aug,\n    method = \"posterior_predict\"\n  )\n  \n  list(\n    model = model_update, \n    residuals = model_resids, \n    data = data_aug\n  )\n  \n}\n\n\n\n\n\n\nHere is the density of the lag one autocorrelation, estimated using residual draws from Bayesian GAMs fitted with and without a first-order autoregressive (AR(1)) term. There is some indication here that the GAM with an AR(1) term is accounting for residual autocorrelation.\n\n\n\nFor a bit more verification, I fitted a similar GAM to a simulated dataset, generated by adding an AR(1) series to a nonlinear trend, as follows:\n\n\nlcl <- 10 # lower censoring limit\n\nsimdat <- withr::with_seed(101, {\n  tibble(\n  x = 1:200,\n  y_t = 1e-3 * x + 1e-4 * x ^ 2,\n  e = arima.sim(list(ar = .5), length(y_t)) %>% \n    as.numeric(),\n  y = y_t + e + 10,\n  y_star = pmax(y, lcl),\n  ci = if_else(y < lcl, -1, 0)\n)\n})\n\n\n\nAgain, I fitted the GAM—and the equivalent model without an AR(1) term—using brms:\n\n\n\n\n\nmodel_simdat <- brm(\n  bf(y_star | cens(ci) ~ s(x) + ar(time = x, p = 1)),\n  data = simdat,\n  seed = 124,\n  # save the model:\n  file = here::here(paste(file_path, \"model_simdat\", sep = \"/\")),\n  cores = 4,\n  control = list(adapt_delta = .99)\n)\n\n\n\nHere are the simulated data and the fitted model:\n\n\n\n\n\n\nAnd here is the ECDF overlay described above:\n\n\n\n\n\n\nThe first-order autocorrelation estimate from the simulated residuals suggests that the model is accounting for autocorrelation in the residuals, as expected:\n\n\n\nAnd the estimate of the AR(1) term is 0.57, which is similar to true value of 0.5.\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\n———. 2018. “Advanced Bayesian Multilevel Modeling with the R Package brms.” The R Journal 10 (1): 395–411. https://doi.org/10.32614/RJ-2018-017.\n\n\nLee, Lopaka. 2020. NADA: Nondetects and Data Analysis for Environmental Data. https://CRAN.R-project.org/package=NADA.\n\n\nStan Development Team. 2021. “RStan: The R Interface to Stan.” https://mc-stan.org/.\n\n\nWang, Chao, and Kung-Sik Chan. 2018. “Quasi-Likelihood Estimation of a Censored Autoregressive Model With Exogenous Variables.” Journal of the American Statistical Association 113 (523): 1135–45. https://doi.org/10.1080/01621459.2017.1307115.\n\n\n\n\n",
    "preview": "posts/2021-10-08-diagnostics-for-censored-autoregressive-models-fit-with-brms/diagnostics-for-censored-autoregressive-models-fit-with-brms_files/figure-html5/ecdf-sim-1.png",
    "last_modified": "2022-01-21T16:00:42-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-08-a-brownification-pause/",
    "title": "A brownification pause?",
    "description": "Censored autoregression with a smoothed time covariate.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-09-08",
    "categories": [],
    "contents": "\n\n\n\nI recently came across a new paper titled “Brownification on hold: What traditional analyses miss in extended surface water records” (Eklöf et al. 2021). In it, the authors argue that linear trend analysis misses important nonlinearities in surface water quality records. Specifically, the paper suggests that surface water browning—increases in coloured organic matter—has paused in recent decades and that generalized additive modeling would be better suited than linear regression to describe that pause.\nEarlier this year I cowrote an article (Redden et al. 2021) describing a linear trend analysis of surface water time series in Nova Scotia. We found strong evidence of browning in many surface waters, but I thought I’d briefly revisit that analysis here to see if we missed anything that a nonlinear trend analysis wouldn’t. This isn’t meant to be a full reanalysis of the data, but it might point in the direction that a future analysis would take.\nThere is a significant roadblock to fitting generalized additive models (GAMs) to the data from our paper: GAMs—at least, as they are implemented in the popular R package mgcv—do not allow censored autoregression (models of autocorrelated time series where part of the series is censored).\nOne option for censored autoregression is the carx package in R. It’s primary function, carx::carx(), also permits a matrix of external covariates that can include a basis expansion—making it possible to fit a GAM to a left-censored, autocorrelated time series.\nIn this post I demonstrate the package on a single measurement series, one of many we used in Redden et al. (2021) The data are sourced from Environment Canada, as described in the paper.\n\n\n\n\n\n\nIn the series, 10% of apparent colour values—those below 5 Pt-Co—are left-censored. To generate a regularly spaced time series, I aggregated the data into semesters by taking medians, and I excluded values collected before 1983. When aggregation required taking the midpoint of a left-censored and an observed value, the output was left-censored at the midpoint between the observed value and the censoring limit.\nI chose to handle missing values by left-censoring them at a limit of positive infinity, as implemented in carx(). The response was log-transformed prior to fitting the model, and the matrix of covariates comprised a cubic regression spline basis expansion of the time variable using splines::bs() with 4 degrees of freedom. The model also included semester as a binary covariate to account for seasonal variation.\n\n\n\nAs a point of comparison, I fit a separate censored autoregression with a linear time covariate. Here are the linear model and GAM predictions along with 95% confidence intervals on the predicted values:\n\n\n\nWhile the GAM does appear to track the series slightly better, the linear model yielded an AIC of -44, whereas the cubic regression spline model yielded a larger AIC of -39.\nBoth models yielded residuals with no obvious deviations from whiteness. For comparison, equivalent models fit using the function lm() (no autoregression) are shown as well. Autocorrelation at lag 1 is notably lower in the residuals from the censored AR(1) models.\n\n\n\nBoth autoregressions yielded residuals that were approximately Gaussian, albeit with somewhat fatter tails than expected (the carx method is robust against mild departures from normality (Wang and Chan 2017)).\n\n\n\nThe variance of the residuals was also reasonably constant, apart from a few outliers and perhaps a slight reduction in later years.\n\n\n\nBut overall, there’s not much evidence here that the GAM describes this particular series any better than the linear model. And a GAM fit using brms (“Bayesian Regression Models using Stan”)—which accomodates both left-censoring and autoregression—yielded similar results. Here is a sketch of the model one might fit using brms::brm(). N.B., missing values are handled differently here: see this vignette.\nWhile I haven’t fully evaluated the fit of this model, it doesn’t suffer from any major issues in terms of convergence. And it is worth noting that it yields a smooth curve that differs little from the censored autogregression with linear time covariate. Fitted values from the latter model—including the AR(1) component—are superimposed in red on the following plot.\n\n\nmodel_in_brms <- model_in %>% \n  mutate(\n    # replace missing values in the censoring indicator with 0 (uncensored)\n    ci = replace_na(ci, 0)\n  )\n\nmodel_brms <- brm(\n  bf(log(value) | cens(ci) + mi() ~\n       s(numeric_date) + sem + ar(time = numeric_date, p = 1)),\n  data = model_in_brms,\n  control = list(adapt_delta = .999),\n  seed = 3152,\n  file = \"model_brms\"\n)\n\n\n\n\n\n\nThe brms and carx models also yielded similar estimates of the AR(1) parameter: 0.21 for the censored autoregression (linear time covariate) and 0.28 for the Bayesian regression model. In percentage terms, the two approaches differ mainly in the predictions over the first part of the series, where most of the censored observations were recorded.\n\n\n\nEklöf, Karin, Claudia von Brömssen, Nino Amvrosiadi, Jens Fölster, Marcus B. Wallin, and Kevin Bishop. 2021. “Brownification on Hold: What Traditional Analyses Miss in Extended Surface Water Records.” Water Research 203 (September): 117544. https://doi.org/10.1016/j.watres.2021.117544.\n\n\nRedden, David J, Benjamin F Trueman, Dewey W Dunnington, Lindsay E Anderson, and Graham Gagnon. 2021. “Chemical Recovery and Browning of Nova Scotia Surface Waters in Response to Declining Acid Deposition.” Environmental Science: Processes & Impacts, 10.1039.D0EM00425A. https://doi.org/10.1039/D0EM00425A.\n\n\nWang, Chao, and Kung-Sik Chan. 2017. “Carx: An R Package to Estimate Censored Autoregressive Time Series with Exogenous Covariates.” The R Journal 9 (2): 213. https://doi.org/10.32614/RJ-2017-064.\n\n\n\n\n",
    "preview": "posts/2021-09-08-a-brownification-pause/a-brownification-pause_files/figure-html5/plot-carx-1.png",
    "last_modified": "2021-12-27T15:28:48-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 864
  },
  {
    "path": "posts/2021-08-13-new-preprint/",
    "title": "New preprint!",
    "description": "Aluminum in drinking water can interact with orthophosphate, increasing lead solubility.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [],
    "contents": "\nRecently, my coauthors and I used a preprint server (chemRxiv) to share an early draft of a research paper for the first time. Preprints are not very common in my field, but I imagine that will change in the future as their advantages become clear. The paper, “Seasonal lead release to drinking water and the effect of aluminum” (Trueman et al. 2021), explores the role of aluminum in limiting the availability of orthophosphate for lead corrosion control in drinking water systems.\nOrthophosphate works by forming an insoluble precipitate with lead, but precipitation with other metals can limit its effect on lead solubility. Here, I reproduce some of the code included in the paper to account for aluminum phosphate precipitation in calculating equilibrium lead solubility. Solubility modeling is done here using pbcusol, an interface for PHREEQC in R that is geared specifically to lead and copper solubility predictions. I also extend the analysis to include other metals that might precipitate with orthophosphate and interfere with lead phosphate formation.\nThe concentrations of interfering aluminum, calcium, manganese, and iron have been chosen to approximate typical ranges, although more extreme values are certainly possible or even likely. A caveat: the phases that precipitate in the model and limit available orthophosphate—variscite (AlPO4·2H2O), vivianite (Fe3(PO4)2·8H2O, CaHPO4, and MnHPO4—may not be the phases that precipitate in drinking water systems.\nTo reproduce the model, load the necessary packages and define a few chemical equations that are not included in the default (pbcusol) database.\n\n\nlibrary(\"tidyverse\")\n# remotes::install_github(\"bentrueman/pbcusol)\nlibrary(\"pbcusol\") \n\nsolids <- list(\n  \"Variscite\",\n  \"AlPO4:2H2O = Al+3 + PO4-3 + 2H2O\",\n  \"log_k\" = -22.36, # https://doi.org/10.1016/j.gca.2010.10.012\n  \"CaHPO4\",\n  \"CaHPO4 = Ca+2 + H+ + PO4-3\",\n  \"log_k\" = -19.275 # from phreeqc::minteq.v4.dat\n)\n\naqueous_species <- list( \n  # from https://doi.org/10.1016/j.gca.2010.10.012 and \n  # https://doi.org/10.1080/09593332708618735\n  \"HPO4-2 + Al+3 = AlHPO4+\",\n  \"log_k\" = 7.4,\n  \"HPO4-2 + H+ + Al+3 = AlH2PO4+2\",\n  \"log_k\" = 3.1\n)\n\n\n\nThen, define a couple of functions that we will use to build the model. The first is a wrapper around pbcusol::eq_sol_fixed() that saves having to code the same arguments repeatedly. The second builds a list of the arguments to pbcusol::eq_sol_fixed() that change across iterations.\n\n\npb_sol_custom <- function(interference = \"Variscite\", ...) {\n      pbcusol::eq_sol_fixed(element = \"Pb\",\n        ph = 7.3, dic = 5,  \n        Na = 10 / chemr::mass(\"Na\"),\n        eq_phase_components = rlang::list2(!!interference := c(0, 0)),\n        new_species = aqueous_species,\n        new_phase = solids,\n        ...\n    ) %>% \n    select(pb_ppb)\n}\n\nbuild_args <- function(x) {\n  list(x$metal_conc / chemr::mass(x$metal)) %>% \n    set_names(nm = x$metal) %>% \n    c(list(\n      phase = x$phase,\n      interference = x$interference,\n      phosphate = x$po4\n    ))\n}\n\n\n\nLead solubility is calculated over a grid of input values in parallel, using future::plan().\n\n\ngrid_size <- 10\n\nfuture::plan(\"multisession\") # parallel iteration\n\nout <- crossing(\n  phase = c(\"Hxypyromorphite\", \"Cerussite\"),\n  po4 = seq(0.1, 2, length.out = grid_size),\n  metal = c(\"Ca\", \"Al\", \"Mn\", \"Fe\"), \n  metal_conc = seq(0, .5, length.out = grid_size)\n) %>% \n  mutate(\n    interference = fct_recode(metal, \n      \"Variscite\" = \"Al\", \"MnHPO4(C)\" = \"Mn\", \n      \"Vivianite\" = \"Fe\", \"CaHPO4\" = \"Ca\"\n    ) %>% \n      as.character(),\n    # adjust metal concentrations to approximate typical values:\n    metal_conc = case_when(\n      metal == \"Ca\" ~ metal_conc * 200,\n      metal == \"Fe\" ~ metal_conc * 2,\n      metal == \"Mn\" ~ metal_conc * .5,\n      TRUE ~ metal_conc\n    )\n  ) %>% \n  rowid_to_column() %>% \n  group_by(rowid) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    args = map(data, build_args),\n    pb_ppb = furrr::future_map(args, ~ do.call(pb_sol_custom, .x))\n  )\n\n\n\nAnd here are the results. At pH 7.3 and a dissolved inorganic carbon concentration of 5 mg C L-1, all four metals impact lead solubility to some extent, and the predicted effect of manganese is largest.\n\n\n\n\n\n\nTrueman, Benjamin F, Aaron Bleasdale-Pollowy, Javier A Locsin, Jessica L Bennett, Wendy H Krkošek, and Graham A Gagnon. 2021. “Seasonal Lead Release to Drinking Water and the Effect of Aluminum.” Preprint. Chemistry. https://doi.org/10.33774/chemrxiv-2021-59nd6-v2.\n\n\n\n\n",
    "preview": "posts/2021-08-13-new-preprint/new-preprint_files/figure-html5/plot-1.png",
    "last_modified": "2021-08-16T16:21:53-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/",
    "title": "Comparing water quality time series using a generalized additive mixed model",
    "description": "Revisiting work from 2016 to better model time series with non-linear trends.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-07",
    "categories": [],
    "contents": "\nIn a 2016 paper (Trueman and Gagnon 2016), I evaluated the effect of cast iron distribution mains on the lead concentrations due to lead pipes downstream from those mains. This question has relevance for minimizing lead in drinking water and for prioritizing lead pipe replacement; if a lead pipe is connected to an unlined iron distribution main, lead levels reaching the consumer are likely to be higher.\nIn the paper, I used the arima() function in R with a matrix of external regressors to account for the effect of the distribution main and autocorrelation in the time series of lead concentrations. But linear regression was only a rough approximation of the concentration time series’ behaviour, and I think using a generalized additive model would have been a better choice. Here, I revisit those data, using mgcv::gamm() to fit a generalized additive mixed model and nlme::corCAR1() to include a continuous time first-order autoregressive error structure.\nFirst, I built the model, using s() to fit a separate smooth to each category of lead time series. (The categories are defined by the distribution main—PVC or cast iron—and the lead pipe configuration—full lead or half lead, half copper.) In this model, the smooths differ in their flexibility and shape (Pedersen et al. 2019). They are centered, so the grouping variables are added as main effects (see the documentation for mgcv::s()). I use tidyr::nest() to allow for list columns that include the model and predicted values along with the data.\n\n\nfe_gam <- jdk_pl %>% \n  filter(fraction == \"total\") %>% \n  mutate_if(is.character, factor) %>% \n  mutate(lsl_grp = interaction(pipe_config, main)) %>% \n  arrange(fraction, lsl, time_d) %>% \n  group_by(fraction) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    model = map(\n      data,\n      ~ mgcv::gamm(\n        log(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config, # model I\n        correlation = nlme::corCAR1(form = ~ time_d | lsl),\n        method = \"REML\",\n        data = .x\n      )\n    )\n  )\n\n\n\nNext, I predicted from the model over the range of x values, and constructed a pointwise (approximate) 95% confidence band using the standard errors of the fitted values.\n\n\nfe_gam <- fe_gam %>% \n  mutate(\n    preds = map2(\n      model, data, \n      ~ predict(.x$gam, newdata = .y, se = TRUE)\n    ),\n    preds = map2(\n      preds, model, \n      ~ tibble(fit = .x$fit, se_fit = .x$se.fit) %>% \n        mutate(\n          lwr = fit - 2 * se_fit,\n          upr = fit + 2 * se_fit,\n          fit = fit\n        ) %>% \n        mutate_at(vars(c(fit, lwr, upr)), exp)\n    )\n  )\n\n\n\nHere are the data, the fitted model, and the confidence bands:\n\n\n\nYou’ll notice that some of the smooths—especially the Full LSL/PVC smooth—are smoother than the eye would expect. This is probably because the model is attributing some of the nonlinearity to autocorrelation, something discussed in more detail elsewhere (Simpson 2018).\nThe model does a reasonably good job—but not a perfect job—accounting for autocorrelation in the time series. “Raw” and “normalized” residuals are defined in the help file to nlme::residuals.lme() under type. Essentially, raw residuals represent the difference between the observed and fitted values, while normalized residuals account for the estimated error structure. The grey shaded band in the figure below represents a 95% confidence interval on the autocorrelation of white Gaussian noise.\n\n\n\nThe natural log transformation yields a model with residuals that are approximately normal.\n\n\n\nFinally, let’s have a look at the model summary. The effect of the distribution main is statistically significant, as is the effect of pipe configuration (which we’re less concerned about here). Based on the retransformed coefficient (exponentiating and subtracting one), the model estimates that lead release is 51% lower when the distribution main supplying the lead pipe is plastic as opposed to iron—an important result given the limited resources available to replace lead drinking water pipes.\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config\n\nParametric coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.44408    0.07202  75.587   <2e-16 ***\nmainpvc                -0.70833    0.08333  -8.501   <2e-16 ***\npipe_configpartial lsl -0.84626    0.08333 -10.156   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                    edf Ref.df      F p-value    \ns(time_d):lsl_grpfull lsl.iron    1.000  1.000  7.718 0.00558 ** \ns(time_d):lsl_grppartial lsl.iron 1.000  1.000  9.904 0.00170 ** \ns(time_d):lsl_grpfull lsl.pvc     1.000  1.000 22.649   3e-06 ***\ns(time_d):lsl_grppartial lsl.pvc  3.444  3.444  3.345 0.00880 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.564   \n  Scale est. = 0.33246   n = 954\n\n\n\n\nPedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross. 2019. “Hierarchical Generalized Additive Models in Ecology: An Introduction with Mgcv.” PeerJ 7 (May): e6876. https://doi.org/10.7717/peerj.6876.\n\n\nSimpson, Gavin L. 2018. “Modelling Palaeoecological Time Series Using Generalised Additive Models.” Frontiers in Ecology and Evolution 6 (October): 149. https://doi.org/10.3389/fevo.2018.00149.\n\n\nTrueman, Benjamin F., and Graham A. Gagnon. 2016. “Understanding the Role of Particulate Iron in Lead Release to Drinking Water.” Environmental Science & Technology 50 (17): 9053–60. https://doi.org/10.1021/acs.est.6b01153.\n\n\n\n\n",
    "preview": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models_files/figure-html5/plot-1.png",
    "last_modified": "2021-08-04T14:57:00-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 672
  },
  {
    "path": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/",
    "title": "Non-parametric matched pair testing with left-censored data",
    "description": "Comparing two groups of measurements when some values are below one or multiple detection limit(s).",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nThere are relatively few options in R for comparing matched pairs in two groups with left-censored data. And while NADA2::cen_signedranktest() is an excellent tool, I wrote the following function as another. It implements the Paired Prentice-Wilcoxon test, as described in Helsel (2012). I should also acknowledge the USGS’ orphaned package smwrQW (Lorenz 2017) for its version of the same test.\n\n\nlibrary(\"tidyverse\")\nlibrary(\"survival\")\n\nppw_test <- function(\n  x, # numeric\n  y, # numeric\n  x_cen, # logical \n  y_cen, # logical \n  alternative = \"two.sided\", # either \"two.sided\", \"greater\" (x > y), or \"less\" (x < y)\n  flip = TRUE\n) {\n  \n  if(length(x) != length(y))\n    stop(\"Lengths of x and y must be the same for paired data.\")\n  \n  if(any(c(x, y) < 0))\n    stop(\"Negative values in x or y.\")\n  \n  valid_alternative <- pmatch(alternative, c(\"two.sided\", \"greater\", \"less\"))\n  \n  if(is.na(valid_alternative))\n    stop('Invalid choice for alternative, must match \"two.sided\", \"greater\", or \"less\"')\n\n  test_input <- tibble(x_val = x, y_val = y, x_cen, y_cen) %>% \n    na.omit() %>% \n    rowid_to_column() %>% \n    pivot_longer(\n      cols = -rowid,\n      names_to = c(\"group\", \".value\"),\n      names_pattern = \"(.+)_(.+)\"\n    ) %>% \n    mutate(\n      cen = as.numeric(!cen), # 0 is censored, 1 is observed\n      # flip data so that smallest observation becomes longest \"survival time\":\n      # N.B., this rounds the flipped data to 6 decimal places\n      flipped = if(flip) {max(val) + 1 - val} else val,\n      flipped = round(flipped, 6)\n    ) %>% \n    left_join(\n      # estimate survival function:\n      survival::survfit(survival::Surv(flipped, cen) ~ 1, data = .) %>% \n        broom::tidy() %>% \n        mutate(time = round(time, 6)),\n      by = c(\"flipped\" = \"time\")\n    ) %>% \n    mutate(score = if_else(cen == 1, 1 - 2 * estimate, 1 - estimate)) %>% \n    pivot_wider(id_cols = rowid, names_from = group, values_from = score) %>% \n    mutate(d = x - y) %>% \n    summarize(\n      z_ppw = sum(d) / sqrt(sum(d ^ 2)),\n      p_val = if(alternative == \"two.sided\") {2 * pnorm(abs(z_ppw), lower.tail = FALSE)} else\n        if(alternative == \"greater\") {pnorm(-z_ppw, lower.tail = FALSE)} else\n          if(alternative == \"less\") {pnorm(z_ppw, lower.tail = FALSE)} else\n            \"alternative hypothesis is invalid\"\n    )\n  \n  list(\"statistic\" = test_input$z_ppw, \"p.value\" = test_input$p_val)\n  \n}\n\n\n\n\n\n\nThe ppw_test() function works like this:\n\n\nwithr::with_seed(450, { # generate two random normal variables, with left-censoring:\n  tibble( \n    x = rnorm(10, 3, 1),\n    y = rnorm(10, 3, 1),\n    x_cen = x < 2,\n    y_cen = y < 2\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen))\n\n\n$statistic\n[1] -0.5628925\n\n$p.value\n[1] 0.5735081\n\nIt also does one-sided tests:\n\n\nwithr::with_seed(23, {\n  tibble( \n    x = rnorm(10, 10, 1),\n    y = rnorm(10, 5, 1),\n    x_cen = x < 10,\n    y_cen = y < 5\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen, alternative = \"greater\")) \n\n\n$statistic\n[1] -2.918231\n\n$p.value\n[1] 0.001760118\n\nThe following code tests that ppw_test() gives the expected result when applied to the data in Table 9.7 of Helsel (2012). First, here are the data:\n\n\n\n\n\n\nAnd here are the test results. These are close to the values in Helsel (2012), but not exactly the same. I suspect there are a few typos in the table, which may have something to do with it. For example, while most of the scores calculated by ppw_test() are consistent with those reported in the table, line 11, column 3 list a score of 0.55 for the second-highest value in column 1, while ppw_test() calculates a score of -0.54, much closer to the value of -0.67 corresponding to the largest value in column 1. There is a similar problem on line 12.\n\n\nhelsel %>% \n  mutate(\n    june_cen = str_detect(june, \"<\"),\n    sept_cen = str_detect(september, \"<\"),\n  ) %>% \n  mutate_if(is.character, ~ as.numeric(str_remove(.x, \"<\"))) %>% \n  with(ppw_test(june, september, june_cen, sept_cen, \"less\"))\n\n\n$statistic\n[1] 3.012394\n\n$p.value\n[1] 0.001295979\n\n\n\n\n\n\n\nHelsel, Dennis R. 2012. Statistics for Censored Environmental Data Using Minitab and R. 2nd ed. Wiley Series in Statistics in Practice. Hoboken, N.J: Wiley.\n\n\nLorenz, Dave. 2017. “SmwrQW–an R Package for Managing and Analyzing Water-Quality Data, Version 0.7.9.” Open File Report. U.S. Geological Survey.\n\n\n\n\n",
    "preview": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/non-parametric-matched-pair-testing-with-left-censored-data_files/figure-html5/helsel-plot-1.png",
    "last_modified": "2021-07-28T17:16:05-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 576
  },
  {
    "path": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/",
    "title": "Peak detection for qualitative XRD analysis in R",
    "description": "An improved workflow for visualizing X-ray diffraction data in R.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-29",
    "categories": [],
    "contents": "\n\n\n\nA important aspect of understanding inorganic contaminant mobility in drinking water systems is identifying the solid phases that exist on the interior surfaces of pipes. This is frequently done with X-ray diffraction (XRD), and I am often preparing figures to communicate XRD data. This post provides an example workflow for displaying XRD data and automatically labelling the peaks so that readers can easily attribute them to the appropriate solid phases. It makes use of the peak detection function peak_maxima() in my field-flow fractionation data analysis package fffprocessr.\nFor this example, we’ll use an XRD pattern representing a mixed lead carbonate phase containing cerussite (PbCO3) and hydrocerussite (Pb3(CO3)2(OH)2). Here are the data, along with a couple of quick plotting functions to avoid repetition later on:\n\n\nlabel_axes <- function(...) {\n  labs(\n    x = expression(\"2\"*theta~\"(Cu K\"*alpha*\")\"),\n    y = \"Intensity\",\n    ...\n  )\n}\n\ncustom_colour_scale <- function(n = 3, ...) {\n  scale_colour_manual(values = wesanderson::wes_palette(\"Zissou1\", n)[c(1, n)])\n}\n\ndata %>%\n  ggplot(aes(two_theta, intensity)) +\n  geom_line() +\n  label_axes()\n\n\n\n\nFirst we need a quick and dirty method of determining the order in which phases should appear in the plot—interpretation is easiest when the standard that matches the data best appears closest to it.\n\n\nxrd_corr <- function(run, standard) {\n  list( \n      sample = run, \n      std = standard\n    ) %>% \n    bind_rows(.id = \"type\") %>% \n    pivot_wider(id_cols = two_theta, names_from = type, values_from = intensity) %>% \n    group_by(two_theta = round(two_theta)) %>% \n    summarize(sample = median(sample, na.rm = TRUE), std = median(std, na.rm = TRUE)) %>% \n    ungroup() %>% \n    with(cor(sample, std, use = \"complete\", method = \"pearson\"))\n}\n\nimportance <- stds %>% \n  distinct(phase) %>% \n  pull(phase) %>% \n  set_names() %>% \n  map_dfc(\n    ~ xrd_corr(\n        run = data,\n        standard = filter(stds, phase == .x) %>% distinct()\n    )\n  ) %>% \n  pivot_longer(everything(), names_to = \"phase\", values_to = \"r\") %>% \n  arrange(desc(r))\n\n\n\nHere are the data with the standard patterns for cerussite and hydrrocerussite. Cerussite is a better match to the data and so it is plotted closest to the data.\n\n\nordered_stds <- stds %>% \n  mutate(phase_f = factor(phase) %>% fct_relevel(importance$phase) %>% as.numeric())\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nWe detect peaks in the pattern using fffprocessr::peak_maxima().\n\n\npeaks_detected <- data %>% \n  peak_maxima(\n    peaks = 23, n = 1, method = \"sigma\", \n     x_var = \"two_theta\", y_var = \"intensity\",\n    group_vars = NULL\n  )\n\n\n\nThen, we need a function to assign the detected peaks to the appropriate standard.\n\n\nassign_peaks <- function(sample, standard, tol = 1, phases) { \n  sample %>% \n    mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n    right_join(\n      standard %>% \n        filter(phase %in% phases) %>% \n        mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n        select(two_theta, two_theta_rnd, phase), \n      by = \"two_theta_rnd\", suffix = c(\"\", \"_std\")\n    ) %>% \n    group_by(two_theta = round(two_theta_std, 1), phase) %>%\n    summarize(intensity = max(intensity)) %>%\n    ungroup()\n}\n\npeaks_idd <- bind_rows(\n  assign_peaks(peaks_detected, stds, phases = importance$phase[1], tol = .5),\n  assign_peaks(peaks_detected, stds, phases = importance$phase[2], tol = .5)\n)\n\n\n\nFinally, we add the identified peaks to the plot:\n\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  geom_point(\n    data = peaks_idd,\n    aes(col = phase)\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nAnd there we have it—the XRD data with and the appropriate standard patterns, with each peak in the data labelled according to the corresponding peak in one of the standards. Of course, this is a relatively crystalline sample, and the signal-to-noise ratio is high; in a future post I’ll test this workflow out on some noisy XRD data.\n\n\n\n",
    "preview": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/peak-detection-for-qualitative-xrd-analysis-in-r_files/figure-html5/data-w-peaks-1.png",
    "last_modified": "2021-10-22T10:04:53-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 672
  },
  {
    "path": "posts/2021-06-18-lead-solubility-prediction-using-shiny/",
    "title": "Lead solubility prediction using R and Shiny",
    "description": "An R package and Shiny app for PHREEQC-based equilibrium lead and copper solubility prediction.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-18",
    "categories": [],
    "contents": "\nRecently, I’ve been experimenting with the excellent R package tidyphreeqc (Dunnington 2019), which provides a convenient interface for generating PHREEQC input files (Charlton and Parkhurst 2011; Parkhurst and Appelo 2013). tidyphreeqc incorporates easily into my workflow, and it’s found its way into several recent publications (Li et al. 2021, 2020).\nI’ve also used it to build a separate package, pbcusol, that handles lead and copper solubility prediction specifically, with curated thermodynamic data and some domain specific features implemented in what is hopefully a straightforward collection of functions. pbcusol is implemented as a Shiny app, available here.\npbcusol can be used to generate lead and copper solubility predictions that are comparable with those found in literature (Schock, Wagner, and Oliphant 1996), as detailed in the package README on GitHub. (They’re reproduced here using the wesanderson package for the colour palette (Ram and Wickham 2018).)\n\n\n\n\n\n\npbcusol can also be used to generate predicted lead and copper solubility in the presence of humic substances. This is also outlined in the package README, but here is an example of the type of visualization that can be generated, along with the code necessary to reproduce the model output. Of course, these predictions should be properly validated—more on that later.\n\n\ngrid_dim <- 25 # dimension of solubility grid square\n\nfuture::plan(\"multisession\")\n\nnom_grid <- crossing(\n  pH_in = seq(6.5, 10.5, length.out = grid_dim),\n  nom_in = seq(0, 3e-3, length.out = grid_dim)\n) %>% \n  rowid_to_column() %>% \n  group_by(rowid) %>% \n  nest() %>% \n  ungroup() %>%  \n  mutate(\n    model = furrr::future_map(data, \n      ~ with(.x, \n        pb_sol_wham(\n          ph = pH_in, dic = 5, phase = \"Hydcerussite\", \n          Na = 10 / chemr::mass(\"Na\"), mass_ha = nom_in\n        )\n      )\n    )\n  )\n\n\n\n\n\n\n\n\n\nCharlton, S. R., and D. L. Parkhurst. 2011. “Modules Based on the Geochemical Model Phreeqc for Use in Scripting and Programming Languages.” Computers & Geosciences 37: 1653–63. http://dx.doi.org/10.1016/j.cageo.2011.02.005.\n\n\nDunnington, Dewey. 2019. tidyphreeqc: Tidy Geochemical Modeling Using Phreeqc. https://github.com/paleolimbot/tidyphreeqc.\n\n\nLi, Bofu, Benjamin F. Trueman, Javier M. Locsin, Yaohuan Gao, Mohammad Shahedur Rahman, Yuri Park, and Graham A. Gagnon. 2021. “Impact of Sodium Silicate on Lead Release from Lead(II) Carbonate.” Environmental Science: Water Research & Technology 7 (3): 599–609. https://doi.org/10.1039/D0EW00886A.\n\n\nLi, Bofu, Benjamin F. Trueman, Mohammad Shahedur Rahman, and Graham A. Gagnon. 2020. “Controlling Lead Release Due to Uniform and Galvanic Corrosion—an Evaluation of Silicate-Based Inhibitors.” Journal of Hazardous Materials, December, 124707. https://doi.org/10.1016/j.jhazmat.2020.124707.\n\n\nParkhurst, D. L., and C. A. J. Appelo. 2013. Description of Input and Examples for Phreeqc Version 3–a Computer Program for Speciation, Batch-Reaction, One-Dimensional Transport, and Inverse Geochemical Calculations. Vol. book 6. Techniques and Methods. U.S. Geological Survey. https://pubs.usgs.gov/tm/06/a43.\n\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson Palette Generator. https://CRAN.R-project.org/package=wesanderson.\n\n\nSchock, M. R., I. Wagner, and R. J. Oliphant. 1996. “Corrosion and solubility of lead in drinking water.” In Internal corrosion of water distribution systems, 2nd ed., 131–230. Denver, CO: American Water Works Association Research Foundation.\n\n\n\n\n",
    "preview": "posts/2021-06-18-lead-solubility-prediction-using-shiny/lead-solubility-prediction-using-shiny_files/figure-html5/basic-grid-plot-1.png",
    "last_modified": "2021-08-16T15:55:42-03:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 576
  }
]
