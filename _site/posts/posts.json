[
  {
    "path": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/",
    "title": "Comparing drinking water quality time series using a generalized additive mixed model",
    "description": "Revisiting work from 2016 to better model non-linear time series.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-07",
    "categories": [],
    "contents": "\nIn a 2016 paper (Trueman and Gagnon 2016), I evaluated the effect of cast iron distribution mains on the lead concentrations due to lead pipes downstream from those mains. This question has relevance for minimizing lead in drinking water and for prioritizing lead pipe replacement; if a lead pipe is connected to an unlined iron distribution main, lead levels reaching the consumer are likely to be higher.\nIn the paper, I used the arima() function in R with a matrix of external regressors to account for the effect of the distribution main and autocorrelation in the time series of lead concentrations. But linear regression was only a rough approximation of the concentration time series’ behaviour, and I think using a generalized additive model would have been a better choice. Here, I revisit those data, using mgcv::gamm() to fit a generalized additive mixed model and nlme::corCAR1() to include a continuous time first-order autoregressive error structure.\nFirst, I built the model, using s() to fit a separate smooth to each category of lead time series. (The categories are defined by the distribution main—PVC or cast iron—and the lead pipe configuration—full lead or half lead, half copper.) In this model, the smooths differ in their flexibility and shape (Pedersen et al. 2019). They are centered, so the grouping variables are added as main effects (see the documentation for mgcv::s()). I use tidyr::nest() to allow for list columns that include the model and predicted values along with the data.\n\n\nfe_gam <- jdk_pl %>% \n  filter(fraction == \"total\") %>% \n  mutate_if(is.character, factor) %>% \n  mutate(lsl_grp = interaction(pipe_config, main)) %>% \n  arrange(fraction, lsl, time_d) %>% \n  group_by(fraction) %>% \n  nest() %>% \n  ungroup() %>% \n  mutate(\n    model = map(\n      data,\n      ~ mgcv::gamm(\n        log(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config, # model I\n        correlation = nlme::corCAR1(form = ~ time_d | lsl),\n        method = \"REML\",\n        data = .x\n      )\n    )\n  )\n\n\n\nNext, I predicted from the model over the range of x values, and constructed a pointwise (approximate) 95% confidence band using the standard errors of the fitted values.\n\n\nfe_gam <- fe_gam %>% \n  mutate(\n    preds = map2(\n      model, data, \n      ~ predict(.x$gam, newdata = .y, se = TRUE)\n    ),\n    preds = map2(\n      preds, model, \n      ~ tibble(fit = .x$fit, se_fit = .x$se.fit) %>% \n        mutate(\n          lwr = fit - 2 * se_fit,\n          upr = fit + 2 * se_fit,\n          fit = fit\n        ) %>% \n        mutate_at(vars(c(fit, lwr, upr)), exp)\n    )\n  )\n\n\n\nHere are the data, the fitted model, and the confidence bands:\n\n\n\nYou’ll notice that some of the smooths—especially the “full LSL”/ “PVC” smooth—are smoother than the eye would expect. This is probably because the model is attributing some of the nonlinearity to autocorrelation, something discussed in more detail elsewhere (Simpson 2018).\nThe model does a reasonably good job—but not a perfect job—accounting for autocorrelation in the time series. “Raw” and “normalized” residuals are defined in the help file to nlme::residuals.lme() under type. Essentially, raw residuals represent the difference between the observed and fitted values, while normalized residuals account for the estimated error structure (here, continuous time first-order autoregressive). The grey shaded band in the figure below represents a 95% confidence interval on the autocorrelation of white Gaussian noise.\n\n\n\nThe natural log transformation yields a model with residuals that are approximately normal.\n\n\n\nFinally, let’s have a look at the model summary. The effect of the distribution main is statistically significant, as is the effect of pipe configuration (which we’re less concerned about here). Based on the retransformed coefficient (exponentiating and subtracting one), the model estimates that lead release is 51% lower when the distribution main supplying the lead pipe is plastic as opposed to iron—an important result given the\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config\n\nParametric coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.44408    0.07202  75.587   <2e-16 ***\nmainpvc                -0.70833    0.08333  -8.501   <2e-16 ***\npipe_configpartial lsl -0.84626    0.08333 -10.156   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                    edf Ref.df      F p-value    \ns(time_d):lsl_grpfull lsl.iron    1.000  1.000  7.718 0.00558 ** \ns(time_d):lsl_grppartial lsl.iron 1.000  1.000  9.904 0.00170 ** \ns(time_d):lsl_grpfull lsl.pvc     1.000  1.000 22.649   3e-06 ***\ns(time_d):lsl_grppartial lsl.pvc  3.444  3.444  3.345 0.00880 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.564   \n  Scale est. = 0.33246   n = 954\n\n\n\n\nPedersen, Eric J., David L. Miller, Gavin L. Simpson, and Noam Ross. 2019. “Hierarchical Generalized Additive Models in Ecology: An Introduction with Mgcv.” PeerJ 7 (May): e6876. https://doi.org/10.7717/peerj.6876.\n\n\nSimpson, Gavin L. 2018. “Modelling Palaeoecological Time Series Using Generalised Additive Models.” Frontiers in Ecology and Evolution 6 (October): 149. https://doi.org/10.3389/fevo.2018.00149.\n\n\nTrueman, Benjamin F., and Graham A. Gagnon. 2016. “Understanding the Role of Particulate Iron in Lead Release to Drinking Water.” Environmental Science & Technology 50 (17): 9053–60. https://doi.org/10.1021/acs.est.6b01153.\n\n\n\n\n",
    "preview": "posts/2021-07-07-comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models/comparing-drinking-water-quality-time-series-using-generalized-additive-mixed-models_files/figure-html5/plot-1.png",
    "last_modified": "2021-07-28T16:12:45-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/",
    "title": "Non-parametric matched pair testing with left-censored data",
    "description": "Comparing two groups of measurements when some values are below one or multiple detection limit(s).",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nThere are relatively few options in R for comparing matched pairs in two groups with left-censored data. And while NADA2::cen_signedranktest() is an excellent tool, I wrote the following function as another. It implements the Paired Prentice-Wilcoxon test, as described in Helsel (2012). I should also acknowledge the USGS’ orphaned package smwrQW (Lorenz 2017) for its version of the same test.\n\n\nlibrary(\"tidyverse\")\nlibrary(\"survival\")\n\nppw_test <- function(\n  x, # numeric\n  y, # numeric\n  x_cen, # logical \n  y_cen, # logical \n  alternative = \"two.sided\", # either \"two.sided\", \"greater\" (x > y), or \"less\" (x < y)\n  flip = TRUE\n) {\n  \n  if(length(x) != length(y))\n    stop(\"Lengths of x and y must be the same for paired data.\")\n  \n  if(any(c(x, y) < 0))\n    stop(\"Negative values in x or y.\")\n  \n  valid_alternative <- pmatch(alternative, c(\"two.sided\", \"greater\", \"less\"))\n  \n  if(is.na(valid_alternative))\n    stop('Invalid choice for alternative, must match \"two.sided\", \"greater\", or \"less\"')\n\n  test_input <- tibble(x_val = x, y_val = y, x_cen, y_cen) %>% \n    na.omit() %>% \n    rowid_to_column() %>% \n    pivot_longer(\n      cols = -rowid,\n      names_to = c(\"group\", \".value\"),\n      names_pattern = \"(.+)_(.+)\"\n    ) %>% \n    mutate(\n      cen = as.numeric(!cen), # 0 is censored, 1 is observed\n      # flip data so that smallest observation becomes longest \"survival time\":\n      # N.B., this rounds the flipped data to 6 decimal places\n      flipped = if(flip) {max(val) + 1 - val} else val,\n      flipped = round(flipped, 6)\n    ) %>% \n    left_join(\n      # estimate survival function:\n      survival::survfit(survival::Surv(flipped, cen) ~ 1, data = .) %>% \n        broom::tidy() %>% \n        mutate(time = round(time, 6)),\n      by = c(\"flipped\" = \"time\")\n    ) %>% \n    mutate(score = if_else(cen == 1, 1 - 2 * estimate, 1 - estimate)) %>% \n    pivot_wider(id_cols = rowid, names_from = group, values_from = score) %>% \n    mutate(d = x - y) %>% \n    summarize(\n      z_ppw = sum(d) / sqrt(sum(d ^ 2)),\n      p_val = if(alternative == \"two.sided\") {2 * pnorm(abs(z_ppw), lower.tail = FALSE)} else\n        if(alternative == \"greater\") {pnorm(-z_ppw, lower.tail = FALSE)} else\n          if(alternative == \"less\") {pnorm(z_ppw, lower.tail = FALSE)} else\n            \"alternative hypothesis is invalid\"\n    )\n  \n  list(\"statistic\" = test_input$z_ppw, \"p.value\" = test_input$p_val)\n  \n}\n\n\n\n\n\n\nThe ppw_test() function works like this:\n\n\nwithr::with_seed(450, { # generate two random normal variables, with left-censoring:\n  tibble( \n    x = rnorm(10, 3, 1),\n    y = rnorm(10, 3, 1),\n    x_cen = x < 2,\n    y_cen = y < 2\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen))\n\n\n$statistic\n[1] -0.5628925\n\n$p.value\n[1] 0.5735081\n\nIt also does one-sided tests:\n\n\nwithr::with_seed(23, {\n  tibble( \n    x = rnorm(10, 10, 1),\n    y = rnorm(10, 5, 1),\n    x_cen = x < 10,\n    y_cen = y < 5\n  )\n}) %>% \n  with(ppw_test(x, y, x_cen, y_cen, alternative = \"greater\")) \n\n\n$statistic\n[1] -2.918231\n\n$p.value\n[1] 0.001760118\n\nThe following code tests that ppw_test() gives the expected result when applied to the data in Table 9.7 of Helsel (2012). First, here are the data:\n\n\n\n\n\n\nAnd here are the test results. These are close to the values in Helsel (2012), but not exactly the same. I suspect there are a few typos in the table, which may have something to do with it. For example, while most of the scores calculated by ppw_test() are consistent with those reported in the table, line 11, column 3 list a score of 0.55 for the second-highest value in column 1, while ppw_test() calculates a score of -0.54, much closer to the value of -0.67 corresponding to the largest value in column 1. There is a similar problem on line 12.\n\n\nhelsel %>% \n  mutate(\n    june_cen = str_detect(june, \"<\"),\n    sept_cen = str_detect(september, \"<\"),\n  ) %>% \n  mutate_if(is.character, ~ as.numeric(str_remove(.x, \"<\"))) %>% \n  with(ppw_test(june, september, june_cen, sept_cen, \"less\"))\n\n\n$statistic\n[1] 3.012394\n\n$p.value\n[1] 0.001295979\n\n\n\n\n\n\n\nHelsel, Dennis R. 2012. Statistics for Censored Environmental Data Using Minitab and R. 2nd ed. Wiley Series in Statistics in Practice. Hoboken, N.J: Wiley.\n\n\nLorenz, Dave. 2017. “SmwrQW–an R Package for Managing and Analyzing Water-Quality Data, Version 0.7.9.” Open File Report. U.S. Geological Survey.\n\n\n\n\n",
    "preview": "posts/2021-07-05-non-parametric-matched-pair-testing-with-left-censored-data/non-parametric-matched-pair-testing-with-left-censored-data_files/figure-html5/helsel-plot-1.png",
    "last_modified": "2021-07-28T17:16:05-03:00",
    "input_file": "non-parametric-matched-pair-testing-with-left-censored-data.utf8.md"
  },
  {
    "path": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/",
    "title": "Peak detection for qualitative XRD analysis in R",
    "description": "An improved workflow for visualizing X-ray diffraction data in R.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-29",
    "categories": [],
    "contents": "\n\n\n\nA important aspect of understanding inorganic contaminant mobility in drinking water systems is identifying the solid phases that exist on the interior surfaces of pipes. This is frequently done with X-ray diffraction (XRD), and I am often preparing figures to communicate XRD data. This post provides an example workflow for displaying XRD data and automatically labelling the peaks so that readers can easily attribute them to the appropriate solid phases. It makes use of the peak detection function peak_maxima() in my field-flow fractionation data analysis package fffprocessr.\nFor this example, we’ll use an XRD pattern representing a mixed lead carbonate phase containing cerussite (PbCO3) and hydrocerussite (Pb3(CO3)2(OH)2). Here are the data, along with a couple of quick plotting functions to avoid repetition later on:\n\n\nlabel_axes <- function(...) {\n  labs(\n    x = expression(\"2\"*theta~\"(Cu K\"*alpha*\")\"),\n    y = \"Intensity\",\n    ...\n  )\n}\n\ncustom_colour_scale <- function(n = 3, ...) {\n  scale_colour_manual(values = wesanderson::wes_palette(\"Zissou1\", n)[c(1, n)])\n}\n\ndata %>%\n  ggplot(aes(two_theta, intensity)) +\n  geom_line() +\n  label_axes()\n\n\n\n\nFirst we need a quick and dirty method of determining the order in which phases should appear in the plot—interpretation is easiest when the standard that matches the data best appears closest to it.\n\n\nxrd_corr <- function(run, standard) {\n  list( \n      sample = run, \n      std = standard\n    ) %>% \n    bind_rows(.id = \"type\") %>% \n    pivot_wider(id_cols = two_theta, names_from = type, values_from = intensity) %>% \n    group_by(two_theta = round(two_theta)) %>% \n    summarize(sample = median(sample, na.rm = TRUE), std = median(std, na.rm = TRUE)) %>% \n    ungroup() %>% \n    with(cor(sample, std, use = \"complete\", method = \"pearson\"))\n}\n\nimportance <- stds %>% \n  distinct(phase) %>% \n  pull(phase) %>% \n  set_names() %>% \n  map_dfc(\n    ~ xrd_corr(\n        run = data,\n        standard = filter(stds, phase == .x) %>% distinct()\n    )\n  ) %>% \n  pivot_longer(everything(), names_to = \"phase\", values_to = \"r\") %>% \n  arrange(desc(r))\n\n\n\nHere are the data with the standard patterns for cerussite and hydrrocerussite. Cerussite is a better match to the data and so it is plotted closest to the data.\n\n\nordered_stds <- stds %>% \n  mutate(phase_f = factor(phase) %>% fct_relevel(importance$phase) %>% as.numeric())\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nWe detect peaks in the pattern using fffprocessr::peak_maxima().\n\n\npeaks_detected <- data %>% \n  peak_maxima(\n    peaks = 23, n = 1, method = \"sigma\", \n     x_var = \"two_theta\", y_var = \"intensity\",\n    group_vars = NULL\n  )\n\n\n\nThen, we need a function to assign the detected peaks to the appropriate standard.\n\n\nassign_peaks <- function(sample, standard, tol = 1, phases) { \n  sample %>% \n    mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n    right_join(\n      standard %>% \n        filter(phase %in% phases) %>% \n        mutate(two_theta_rnd = plyr::round_any(two_theta, tol)) %>% \n        select(two_theta, two_theta_rnd, phase), \n      by = \"two_theta_rnd\", suffix = c(\"\", \"_std\")\n    ) %>% \n    group_by(two_theta = round(two_theta_std, 1), phase) %>%\n    summarize(intensity = max(intensity)) %>%\n    ungroup()\n}\n\npeaks_idd <- bind_rows(\n  assign_peaks(peaks_detected, stds, phases = importance$phase[1], tol = .5),\n  assign_peaks(peaks_detected, stds, phases = importance$phase[2], tol = .5)\n)\n\n\n\nFinally, we add the identified peaks to the plot:\n\n\ndata %>% \n  ggplot(aes(two_theta, intensity)) + \n  geom_line() +\n  geom_segment(\n    data = ordered_stds,\n    aes(\n      x = two_theta, xend = two_theta, \n      y = .25 * (0 - phase_f), yend = .25 * (intensity - phase_f), \n      col = phase\n    )\n  ) + \n  geom_point(\n    data = peaks_idd,\n    aes(col = phase)\n  ) + \n  scale_y_continuous(breaks = seq(0, 1, .25)) +\n  label_axes(col = NULL) + \n  custom_colour_scale(4)\n\n\n\n\nAnd there we have it—the XRD data with and the appropriate standard patterns, with each peak in the data labelled according to the corresponding peak in one of the standards. Of course, this is a relatively crystalline sample, and the signal-to-noise ratio is high; in a future post I’ll test this workflow out on some noisy XRD data.\n\n\n\n",
    "preview": "posts/2021-06-29-peak-detection-for-qualitative-xrd-analysis-in-r/peak-detection-for-qualitative-xrd-analysis-in-r_files/figure-html5/plot-funs-1.png",
    "last_modified": "2021-07-28T16:33:41-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-18-lead-solubility-prediction-using-shiny/",
    "title": "Lead solubility prediction using R and Shiny",
    "description": "An R package and Shiny app for PHREEQC-based equilibrium lead and copper solubility prediction.",
    "author": [
      {
        "name": "Ben Trueman",
        "url": {}
      }
    ],
    "date": "2021-06-18",
    "categories": [],
    "contents": "\nRecently, I’ve been experimenting with the excellent R package tidyphreeqc (Dunnington 2019), which provides a convenient interface for generating PHREEQC input files (Charlton and Parkhurst 2011; Parkhurst and Appelo 2013). tidyphreeqc incorporates easily into my workflow, and it’s found its way into several recent publications (Li et al. 2021, 2020).\nI’ve also used it to build a separate package, pbcusol, that handles lead and copper solubility prediction specifically, with curated thermodynamic data and some domain specific features implemented in what is hopefully a straightforward collection of functions. pbcusol is implemented as a Shiny app, available here.\npbcusol can be used to generate lead and copper solubility predictions that are comparable with those found in literature (Schock, Wagner, and Oliphant 1996), as detailed in the package README on GitHub. (They’re reproduced here using the wesanderson package for colour palettes (Ram and Wickham 2018).)\n\n\n\npbcusol can also be used to generate predicted lead and copper solubility in the presence of humic substances. This is also outlined in the package README, but here is an example of the type of visualization that can be generated. Of course, these predictions should be properly validated—more on that later.\n\n\n\n\n\n\nCharlton, S. R., and D. L. Parkhurst. 2011. “Modules Based on the Geochemical Model Phreeqc for Use in Scripting and Programming Languages.” Computers & Geosciences 37: 1653–63. http://dx.doi.org/10.1016/j.cageo.2011.02.005.\n\n\nDunnington, Dewey. 2019. tidyphreeqc: Tidy Geochemical Modeling Using Phreeqc. https://github.com/paleolimbot/tidyphreeqc.\n\n\nLi, Bofu, Benjamin F. Trueman, Javier M. Locsin, Yaohuan Gao, Mohammad Shahedur Rahman, Yuri Park, and Graham A. Gagnon. 2021. “Impact of Sodium Silicate on Lead Release from Lead(II) Carbonate.” Environmental Science: Water Research & Technology 7 (3): 599–609. https://doi.org/10.1039/D0EW00886A.\n\n\nLi, Bofu, Benjamin F. Trueman, Mohammad Shahedur Rahman, and Graham A. Gagnon. 2020. “Controlling Lead Release Due to Uniform and Galvanic Corrosion—an Evaluation of Silicate-Based Inhibitors.” Journal of Hazardous Materials, December, 124707. https://doi.org/10.1016/j.jhazmat.2020.124707.\n\n\nParkhurst, D. L., and C. A. J. Appelo. 2013. Description of Input and Examples for Phreeqc Version 3–a Computer Program for Speciation, Batch-Reaction, One-Dimensional Transport, and Inverse Geochemical Calculations. Vol. book 6. Techniques and Methods. U.S. Geological Survey. https://pubs.usgs.gov/tm/06/a43.\n\n\nRam, Karthik, and Hadley Wickham. 2018. wesanderson: A Wes Anderson Palette Generator. https://CRAN.R-project.org/package=wesanderson.\n\n\nSchock, M. R., I. Wagner, and R. J. Oliphant. 1996. “Corrosion and solubility of lead in drinking water.” In Internal corrosion of water distribution systems, 2nd ed., 131–230. Denver, CO: American Water Works Association Research Foundation.\n\n\n\n\n",
    "preview": "posts/2021-06-18-lead-solubility-prediction-using-shiny/lead-solubility-prediction-using-shiny_files/figure-html5/basic-grid-1.png",
    "last_modified": "2021-07-28T17:12:11-03:00",
    "input_file": {}
  }
]
