---
title: "Comparing drinking water quality time series using a generalized additive mixed model"
description: |
  Revisiting work from 2016 to better model non-linear time series.
author:
  - name: Ben Trueman
    url: {}
date: 07-07-2021
bibliography: references.bib
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("mgcv")
library("gratia")
options(dplyr.summarise.inform = FALSE)
theme_set(theme_bw(14) + theme(strip.background = element_blank()))
pal <- wesanderson::wes_palette("Zissou1", 5)[c(5, 1)]
jdk_pl <- read_csv("trueman_gagnon_2016_pipe_loop_data_cleaned.csv")
```

In a 2016 paper [@trueman_understanding_2016], I evaluated the effect of cast iron distribution mains on the lead concentrations due to lead pipes downstream from those mains. This question has relevance for minimizing lead in drinking water and for prioritizing lead pipe replacement; if a lead pipe is connected to an unlined iron distribution main, lead levels reaching the consumer are likely to be higher.

In the paper, I used the `arima()` function in R with a matrix of external regressors to account for the effect of the distribution main and autocorrelation in the time series of lead concentrations. But linear regression was only a rough approximation of the concentration time series' behaviour, and I think using a generalized additive model would have been a better choice. Here, I revisit those data, using `mgcv::gamm()` to fit a generalized additive mixed model and `nlme::corCAR1()` to include a continuous time first-order autoregressive error structure.

First, I built the model, using `s()` to fit a separate smooth to each category of lead time series. (The categories are defined by the distribution main---PVC or cast iron---and the lead pipe configuration---full lead or half lead, half copper.) In this model, the smooths differ in their flexibility and shape [@pedersen_hierarchical_2019]. They are centered, so the grouping variables are added as main effects (see the documentation for `mgcv::s()`). I use `tidyr::nest()` to allow for list columns that include the model and predicted values along with the data.

```{r model}

fe_gam <- jdk_pl %>% 
  filter(fraction == "total") %>% 
  mutate_if(is.character, factor) %>% 
  mutate(lsl_grp = interaction(pipe_config, main)) %>% 
  arrange(fraction, lsl, time_d) %>% 
  group_by(fraction) %>% 
  nest() %>% 
  ungroup() %>% 
  mutate(
    model = map(
      data,
      ~ mgcv::gamm(
        log(pb_ppb) ~ s(time_d, by = lsl_grp) + main + pipe_config, # model I
        correlation = nlme::corCAR1(form = ~ time_d | lsl),
        method = "REML",
        data = .x
      )
    )
  )

```

Next, I predicted from the model over the range of x values, and constructed a pointwise (approximate) 95% confidence band using the standard errors of the fitted values.

```{r confidence-bands}

fe_gam <- fe_gam %>% 
  mutate(
    preds = map2(
      model, data, 
      ~ predict(.x$gam, newdata = .y, se = TRUE)
    ),
    preds = map2(
      preds, model, 
      ~ tibble(fit = .x$fit, se_fit = .x$se.fit) %>% 
        mutate(
          lwr = fit - 2 * se_fit,
          upr = fit + 2 * se_fit,
          fit = fit
        ) %>% 
        mutate_at(vars(c(fit, lwr, upr)), exp)
    )
  )

```

Here are the data, the fitted model, and the confidence bands:

```{r plot, echo=FALSE, fig.height=3.5}

fe_gam %>% 
  unnest(c(data, preds)) %>% 
  mutate(
    main = fct_recode(main, "Iron" = "iron", "PVC" = "pvc"),
    fraction = str_replace(fraction, "u", "µ") %>% 
      fct_recode("Total" = "total") %>% 
      fct_relevel("Total", after = 0L),
    pipe_config = str_to_sentence(pipe_config) %>% 
      str_replace("lsl", "LSL")
  ) %>% 
  ggplot(aes(time_d, col = main, fill = main)) + 
  facet_wrap(vars(pipe_config)) +
  geom_point(
    data = function(x) x %>% 
      group_by(time_d, main, fraction, pipe_config) %>% 
      summarize(pb_ppb = mean(pb_ppb)),
    aes(y = pb_ppb), alpha = .3, shape = 16, size = 3
  ) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .3, col = NA) +
  geom_line(aes(y = fit, group = interaction(pipe_config, main))) +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  guides(col = guide_legend(override.aes = list(size = 1))) +
  theme(legend.position = "right") +
  labs(
    x = "Days elapsed", 
    y = expression("[Pb] (µg L"^"-1"*")"),
    col = "Distribution\nmain",
    fill = "Distribution\nmain"
  ) 
 
```

You'll notice that some of the smooths---especially the "full LSL"/ "PVC" smooth---are smoother than the eye would expect. This is probably because the model is attributing some of the  nonlinearity to autocorrelation, something discussed in more detail elsewhere [@simpson_modelling_2018].

The model does a reasonably good job---but not a perfect job---accounting for autocorrelation in the time series. "Raw" and "normalized" residuals are defined in the help file to `nlme::residuals.lme()` under `type`. Essentially, raw residuals represent the difference between the observed and fitted values, while normalized residuals account for the estimated error structure (here, continuous time first-order autoregressive). The grey shaded band in the figure below represents a 95% confidence interval on the autocorrelation of white Gaussian noise.

```{r acf, fig.height=2.5, echo=FALSE}

fe_gam %>% 
  mutate(
    resid = map(model, ~ residuals(.x$gam)),
    bound = map(resid, ~ 1.96 / sqrt(length(.x))),
    resid_norm = map(model, ~ residuals(.x$lme, type = "normalized")),
    acf_raw = map(resid, ~ acf(.x, plot = FALSE) %>% with(tibble(lag_raw = lag, acf_raw = acf))),
    acf_norm = map(resid_norm, ~ acf(.x, plot = FALSE) %>% with(tibble(lag_norm = lag, acf_norm = acf)))
  ) %>% 
  unnest(c(acf_raw, acf_norm, bound)) %>%
  select_if(~ !is.list(.x)) %>% 
  pivot_longer(-c(fraction, bound), names_to = c(".value", "type"), names_pattern = "(.+)_(.+)") %>% 
  filter(lag != 0) %>%
  group_by(fraction) %>% 
  ggplot(aes(lag, acf, col = type)) + 
  geom_ribbon(
    aes(x = lag, ymin = -bound, ymax = bound),
    alpha = .2, inherit.aes = FALSE
  ) +
  geom_hline(yintercept = 0) +
  geom_line() + 
  scale_colour_manual(values = pal, labels = c("Normalized", "Raw")) +
  labs(x = "Time lag", y = "Autocorrelation", col = "Residual type")

```

The natural log transformation yields a model with residuals that are approximately normal.

```{r qqplots, fig.height=2.5, echo=FALSE}

fe_gam %>% 
  mutate(resid = map(model, ~ residuals(.x$gam))) %>% 
  unnest(c(data, resid)) %>% 
  ggplot(aes(sample = resid)) + 
  geom_qq(size = 3, alpha = .2, shape = 16) + 
  geom_qq_line(col = "firebrick") + 
  labs(
    x = "Standard normal quantiles",
    y = "Residuals"
  )

```

Finally, let's have a look at the model summary. The effect of the distribution main is statistically significant, as is the effect of pipe configuration (which we're less concerned about here). Based on the retransformed coefficient (exponentiating and subtracting one), the model estimates that lead release is `r signif(abs(100 * (exp(coef(fe_gam$model[[1]]$gam)[2]) - 1)), 2)`% lower when the distribution main supplying the lead pipe is plastic as opposed to iron---an important result given the limited resources available to replace lead drinking water pipes.  

```{r summary, echo=FALSE}
summary(fe_gam$model[[1]]$gam)
```


