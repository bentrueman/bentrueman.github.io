---
title: "A brownification pause?"
description: |
  Censored autoregression with a smoothed time covariate.
author:
  - name: Ben Trueman
    url: {}
date: 09-08-2021
output:
  distill::distill_article:
    self_contained: false
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.height = 3)
library("tidyverse")
library("brms")
library("carx")
theme_set(
  theme_bw(14) + 
    theme(
      legend.position = "bottom",
      strip.background = element_blank()
    )
)
pal <- wesanderson::wes_palette("Zissou1")
```

```{r load, message=FALSE}
ldat <- read_csv("UPPER-SILVER-LAKE.csv")
```

I recently  came across a new paper titled "Brownification on hold: What traditional analyses miss in extended surface water records" [@eklof_brownification_2021]. In it, the authors argue that linear trend analysis misses important nonlinearities in surface water quality records. Specifically, the paper suggests that surface water browning---increases in coloured organic matter---has paused in recent decades and that generalized additive modeling would be better suited than linear regression to describe that pause. 

Earlier this year I cowrote an article [@redden_chemical_2021] describing a linear trend analysis of surface water time series in Nova Scotia. We found strong evidence of browning in many surface waters, but I thought I'd briefly revisit that analysis here to see if we missed anything that a nonlinear trend analysis wouldn't. This isn't meant to be a full reanalysis of the data, but it might point in the direction that a future analysis would take.

There is a significant roadblock to fitting generalized additive models (GAMs) to the data from our paper: GAMs---at least, as they are implemented in the popular R package `mgcv`---do not allow *censored autoregression* (models of autocorrelated time series where part of the series is censored). 

One option for censored autoregression is the `carx` package in R. It's primary function, `carx::carx()`, also permits a matrix of external covariates that can include a basis expansion---making it possible to fit a GAM to a left-censored, autocorrelated time series.

In this post I demonstrate the package on a single measurement series, one of many we used in @redden_chemical_2021 The data are sourced from Environment Canada, as described in the paper.

```{r plot}

ldat %>% 
  mutate(
    cen = if_else(is.na(flag), "Observed", "Left-censored")
  ) %>%
  ggplot(aes(date, value)) + 
  geom_line(
    data = function(x) x %>% 
      filter(date > "1980-01-01")
  ) +
  geom_point(aes(col = cen), size = 3, shape = 16, alpha = .5) +
  scale_color_manual(values = pal[c(3,1)]) +
  labs(x = NULL, y = "Apparent colour (Pt-Co)", col = NULL)

```

```{r model}

ldat_clean <- ldat %>%
  arrange(date) %>% 
  transmute(
    date, 
    yr = lubridate::year(date),
    sem = lubridate::semester(date),
    value,
    ci = if_else(flag == "<", -1, 0),
    ci = replace_na(ci, 0)
  )

model_in <- crossing(
  yr = seq(1983, max(ldat_clean$yr), by = 1),
  sem = 1:2
) %>% 
  left_join(ldat_clean, by = c("yr", "sem")) %>%
  group_by(yr, sem) %>% 
  summarize(
    date = median(date),
    med_ci = median(ci),
    value = median(value)
  ) %>% 
  ungroup() %>% 
  transmute(
    date,
    numeric_date = yr + sem / 2,
    numeric_date = numeric_date - min(numeric_date) + 1,
    sem = factor(sem),
    value,
    ci = sign(med_ci),
    lcl = if_else(ci == -1, value, 5)
  )

spline_df <- 4

```

In the series, `r 100 * signif(mean(ldat_clean$ci == -1), 2)`% of apparent colour values---those below 5 Pt-Co---are left-censored. To generate a regularly spaced time series, I aggregated the data into semesters by taking medians, and I excluded values collected before `r lubridate::year(min(model_in$date, na.rm = TRUE))`. When aggregation required taking the midpoint of a left-censored and an observed value, the output was left-censored at the midpoint between the observed value and the censoring limit.

I chose to handle missing values by left-censoring them at a limit of positive infinity, as implemented in `carx()`. The response was log-transformed prior to fitting the model, and the matrix of covariates comprised a cubic regression spline basis expansion of the time variable using `splines::bs()` with `r spline_df` degrees of freedom. The model also included semester as a binary covariate to account for seasonal variation.

```{r model-carx}

out_tbl <- function(x, y) {
  tibble(
      fit = fitted(x),
      pred = y$fit,
      lwr = y$ci[,1],
      upr = y$ci[,2]
    ) %>%
    mutate(across(c(fit, pred, lwr, upr), exp))
}

model <- model_in %>% 
  nest(data = everything()) %>% 
  mutate(
    # log transform y, lcl; convert to df
    data_df = map(data, ~ mutate(.x, across(c(value, lcl), log)) %>% data.frame()), 
    # formulas:
    formula_lm = map(data_df, ~ formula(value ~ numeric_date + sem)),
    formula_gam = map(data_df,
      ~ formula(value ~ splines::bs(numeric_date, df = spline_df) + sem)
    ),
    # models:
    model_lm = map2(data_df, formula_lm,~ lm(.y, data = .x)),
    model_gam = map2(data_df, formula_gam, ~ lm(.y, data = .x)),
    model_lm_carx = map2(data_df, formula_lm,
      ~ carx(
        .y, data = .x, y.na.action = "as.censored", 
        seed = 431
      )
    ),
    model_gam_carx = map2(data_df, formula_gam,
      ~ carx(.y, data = .x, y.na.action = "as.censored", seed = 4411)
    ),
    # predictions:
    preds_lm = map2(model_lm_carx, data_df,
      ~ predict(.x, newxreg = .y, n.ahead = nrow(.y))
    ),
    preds_gam = map2(model_gam_carx, data_df,
      ~ predict(.x, newxreg = .y, n.ahead = nrow(.y))
    ),
    # output:
    out_lm = map2(model_lm_carx, preds_lm, ~ out_tbl(.x, .y)),
    out_gam = map2(model_gam_carx, preds_gam, ~ out_tbl(.x, .y))
  )

```

As a point of comparison, I fit a separate censored autoregression with a linear time covariate. Here are the linear model and GAM predictions along with 95% confidence intervals on the predicted values:

```{r plot-carx, fig.height=4.5}

list(
  `Linear AR(1)` = unnest(model, c(data, out_lm)),
  `GAM AR(1)` = unnest(model, c(data, out_gam))
) %>% 
  bind_rows(.id = "model_type") %>% 
  select(where(~!is.list(.x))) %>% 
  mutate(
    cen = fct_recode(factor(ci), "Observed" = "0", "Left-censored" = "-1"),
    cen = fct_relevel(cen, "Observed", after = 0L)
  ) %>% 
  ggplot(aes(date)) + 
  facet_wrap(vars(model_type), ncol = 1) +
  geom_ribbon(
    aes(ymin = lwr, ymax = upr, group = sem), 
    alpha = .1, col = NA,
  ) +
  geom_line(aes(y = value), col = "grey75") +
  geom_line(aes(y = pred, linetype = sem)) +
  geom_point(
    data = function(x) x %>%
      filter(!is.na(ci)),
    aes(y = value, col = cen),
    size = 3, shape = 16, alpha = .8
  ) +
  scale_color_manual(values = pal[c(1,3)]) +
  labs(
    x = NULL, col = NULL, shape = NULL, 
    y = "Apparent colour (Pt-Co)",
    linetype = "Semester"
  )

```

While the GAM does appear to track the series slightly better, the linear model yielded an AIC of `r signif(model$model_lm_carx[[1]]$aic, 2)`, whereas the cubic regression spline model yielded a larger AIC of `r signif(model$model_gam_carx[[1]]$aic, 2)`.

Both models yielded residuals with no obvious deviations from whiteness; the grey band denotes $\pm1.96/\sqrt{n}$, the approximate 95% confidence bands on the autocorrelation function of white noise. For comparison, equivalent models fit using the function `lm()` (no autoregression) are shown as well. Autocorrelation at lag 1 is notably lower in the residuals from the censored AR(1) models.

```{r diagnostics}

custom_acf <- function(x, suffix = "_carx") {
  x %>% 
    acf(plot = FALSE, lag.max = 15) %>% 
    broom::tidy() %>% 
    rename_all(~ paste0(.x, suffix))
}

model <- model %>% 
  mutate(
    # residuals:
    resid_lm = map(model_lm, residuals),
    resid_gam = map(model_gam, residuals),
    resid_lm_carx = map(model_lm_carx, residuals),
    resid_gam_carx = map(model_gam_carx, residuals),
    # acf:
    acf_lm = map(resid_lm, ~ custom_acf(.x, "_lm")),
    acf_gam = map(resid_gam, ~ custom_acf(.x, "_gam")),
    acf_lm_carx = map(resid_lm_carx, ~ custom_acf(.x, "_lm.carx")),
    acf_gam_carx = map(resid_gam_carx, ~ custom_acf(.x, "_gam.carx")),
    bound = map(resid_lm_carx, ~ 1.96 / sqrt(length(.x)))
  )

model %>% 
  unnest(c(starts_with("acf"), bound)) %>% 
  select(where(~!is.list(.x))) %>% 
  pivot_longer(
    cols = -bound,
    names_to = c(".value", "name"),
    names_pattern = "(.+)_(.+)"
  ) %>% 
  mutate(
    name = fct_recode(name, 
      "Linear" = "lm",
      "GAM" = "gam",
      " Linear AR(1)" = "lm.carx",
      "GAM AR(1)" = "gam.carx"
    )
  ) %>% 
  filter(lag > 0) %>% 
  ggplot(aes(lag)) + 
  geom_ribbon(aes(ymin = -bound, ymax = bound), alpha = .2) +
  geom_line(aes(y = acf, col = name)) + 
  scale_color_manual(values = pal[c(1,3,4,5)]) +
  labs(x = "Time lag", y = "Autocorrelation function", col = "Model")

```

Both autoregressions yielded residuals that were approximately Gaussian, albeit with somewhat fatter tails than expected (the `carx` method is robust against mild departures from normality [@wang_carx_2017]).

```{r qq, fig.height=3.5}

unnest_pivot <- function(x, regex) {
  x %>% 
    unnest(matches(regex)) %>% 
    select(where(~!is.list(.x))) %>% 
    pivot_longer(starts_with("resid_"), values_to = "resid") 
}

bind_rows(
  unnest_pivot(model, "^resid_[a-z]+$"),
  unnest_pivot(model, "^resid_[a-z]+_[a-z]+$")
) %>% 
  mutate(
    name = fct_recode(name, 
      "Linear" = "resid_lm",
      "GAM" = "resid_gam",
      " Linear AR(1)" = "resid_lm_carx",
      "GAM AR(1)" = "resid_gam_carx"
    )
  ) %>% 
  ggplot(aes(sample = resid)) + 
  facet_wrap(vars(name)) +
  geom_qq() + 
  geom_qq_line() +
  labs(x = "Standard normal quantiles", y = "Model residuals")

```

The variance of the residuals was also reasonably constant, apart from a few outliers and perhaps a slight reduction in later years.

```{r r-vs-time}

unnest_pivot(model, "^resid_[a-z]+_[a-z]+$|^data$")%>% 
  mutate(
    name = fct_recode(name, 
      "Linear AR(1)" = "resid_lm_carx",
      "GAM AR(1)" = "resid_gam_carx"
    )
  ) %>% 
  ggplot(aes(date, resid)) + 
  facet_wrap(vars(name)) +
  geom_point() +
  geom_line(col = "grey", size = .3) + 
  labs(x = NULL, y = "Model residuals")

```

But overall, there's not much evidence here that the GAM describes this particular series any better than the linear model. And a GAM fit using `brms` ("Bayesian Regression Models using Stan")---which accomodates both left-censoring and autoregression---yielded similar results. Here is a sketch of the model one might fit using `brms::brm()`; missing values are left-censored at the maximum observed value of the series. While I haven't fully evaluated the fit of this model, it is worth noting that it yields a smooth curve that differs little from the censored autogregression with linear time covariate. Fitted values from the latter model---including the AR(1) component---are superimposed in red on the following plot.


```{r model-brms, echo=TRUE, results="hide"}

model_in_brms <- model_in %>% 
  mutate(
    # left-censor missing values at the maximum value of the series
    ci = replace_na(ci, -1),
    value = replace_na(value, max(value, na.rm = TRUE)),
    censored = if_else(ci == -1, "left", "none")
  )

model_brms <- brm(
  bf(log(value) | cens(censored) ~
       s(numeric_date) + sem + ar(time = numeric_date, p = 1)),
  data = model_in_brms,
  control = list(adapt_delta = .999),
  seed = 3152
)

```

```{r model-criticism, eval=FALSE}

pairs(model_brms)
plot(model_brms, ask = FALSE)
summary(model_brms)
mcmc_plot(model_brms)
conditional_smooths(model_brms)
pp_check(model_brms, type = "ecdf_overlay") + # improved below
  theme_classic() + 
  labs(y = "ecdf")

```

```{r model-preds, fig.height=4}

preds <- model_in_brms %>% 
  tidybayes::add_predicted_draws(model_brms)

preds %>%
  mutate(
    censored = if_else(censored == "left", "Left-censored", "Observed")
  ) %>%
  ggplot(aes(date)) +
  geom_line(
    data = function(x) x %>%
      distinct(date, value),
    aes(y = value), col = "grey75"
  ) +
  tidybayes::stat_lineribbon(
    aes(y = exp(.prediction), linetype = sem),
    .width = .95,
    alpha = .1,
    fill = "black",
    show.legend = FALSE
  ) +
  tidybayes::stat_lineribbon(
    aes(y = exp(.prediction), linetype = sem, col = "brms"),
    .width = 0,
    show.legend = FALSE
  ) +
  geom_point(
    data = function(x) x %>%
      distinct(date, value, censored) %>%
      filter(!is.na(date)),
    aes(y = value, col = censored),
    size = 3, shape = 16, alpha = .5
  ) +
  geom_line(
    data = model %>%
      unnest(c(data, out_lm)),
    aes(date, fit, linetype = sem, col = "carx"), 
    inherit.aes = FALSE
  ) +
  scale_color_manual(values = c("black", pal[c(5,3,1)])) +
  scale_y_continuous(limits = c(0, 70)) +
  # scale_y_log10() +
  labs(
    x = NULL,
    y = "Apparent colour (Pt-Co)", col = NULL,
    linetype = "Semester"
  )

```

The `brms` and `carx` models also yielded similar estimates of the AR(1) parameter: `r signif(model$model_lm_carx[[1]]$coefficients["AR1"], 2)` for the censored autoregression (linear time covariate) and `r signif(summary(model_brms)$cor_pars$Estimate, 2)` for the Bayesian regression model. In percentage terms, the two approaches differ mainly in the predictions over the first part of the series, where most of the censored observations were recorded.

