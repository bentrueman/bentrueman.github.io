---
title: "Regression models with censored predictors"
description: |
  A short description of the post.
author:
  - name: Ben Trueman
    url: {}
date: 2023-03-13
output:
  distill::distill_article:
    self_contained: false
draft: true
bibliography: references.bib
---

<!-- to do: 
1. get rid of future_map()?
2 standardize map() syntax -->

```{r setup, include=FALSE}
here::i_am("2023-03-13-regression-models-with-censored-predictors/regression-models-with-censored-predictors.Rmd")
knitr::opts_chunk$set(echo = FALSE)
```

Recently, I've encountered several regression problems where the predictors are partially censored. To tack this issue, I turned to Paul B&uuml;rkner's excellent package for Bayesian regression, `brms` [@brms]. Paul's package includes a helpful vignette on handling [missing values](https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html). The general strategy we'll use here is outlined in that vignette: imputation during model fitting by declaring the missing values as parameters in the model. That is, we'll fit a multivariate model that imputes missing values in both the predictor and the response variable. The only problem is that `brms` doesn't handle censored predictors, so we'll have to modify Paul's approach.

We'll need the following packages:

```{r packages, echo=TRUE, message=FALSE}
library("tidyverse")
library("glue")
library("testthat")
library("withr")
library("brms")
library("posterior")
library("rstan")
library("furrr")
plan("multisession") # for parallel processing
options(mc.cores = parallel::detectCores()) # for parallel MCMC chains
```

```{r theme}
theme_set(
  theme_bw(14) + 
    theme(
      legend.position = "bottom",
      strip.background = element_blank()
    )
)
pal <- wesanderson::wes_palette("Zissou1")
```

Then we'll need to simulate some data:

```{r, echo=TRUE}
n <- 50 # total number of observations
lcl <- -1 # lower censoring limit

with_seed(1235, {
  data <- tibble(
    x = rnorm(n, 0, 2),
    y = 2 * x + rnorm(n)
  )
})

data$x_star <- data$x
data$x_star[c(3, 29)] <- NA # add missing values
data$cens_x_star <- replace_na(data$x_star < lcl, FALSE) # censoring indicator
data$x_star <- pmax(data$x_star, lcl) # left-censor values

# train/test split:
train <- 1:25 
data_train <- data[train, ]
data_test <- data[-train, ]
```

Let's plot the simulated data so we can see what we're dealing with. Missing `x` values are shown as horizontal lines, and left-censored x-values are shown as horizontal line segments extending from the left edge of the plot to the censoring limit of -1. The true values corresponding to the censored and missing observations are transparent.

```{r plot}
p1 <- data %>% 
  rowid_to_column() %>% 
  mutate(
    type = if_else(rowid %in% train, "Training data", "Test data"),
    missing = is.na(data$x_star)
  ) %>% 
  ggplot(aes(x, y, col = type)) + 
  scale_color_manual(values = pal[2:3]) +
  geom_point(
    data = . %>% 
      filter(cens_x_star | missing),
    alpha = .4, shape = 16
  ) +
  geom_point(
    data = . %>% 
      filter(!cens_x_star & !missing),
    shape = 16
  ) +
  geom_hline(
    data = . %>% 
      filter(missing),
    aes(yintercept = y, col = type)
  ) +
  geom_segment(
    data = . %>%
      filter(cens_x_star),
    aes(x = -1, xend = -Inf, yend = y)
  ) + 
  labs(col = NULL)
p1
```

The trick at this point is to treat left-censored `x` values as missing values with an upper bound equal to the censoring limit, as describe in the [Stan manual](https://mc-stan.org/docs/2_18/stan-users-guide/censored-data.html). But since, `brms` doesn't do this we'll have to code it ourselves. We'll need two functions: one to modify the data list and another to modify the code that `brms` passes to Stan:

```{r functions-modify, echo=TRUE}
modify_standata <- function(sdata, data, lcl, var) {
  
  if (length(lcl) != length(var)) stop("lengths of 'var' and 'lcl' must be equal")
  
  varstan <- str_remove_all(var, "_")
  
  for(i in seq_len(length(var))) {
    sdata[[paste0("Ncens_", varstan[i])]] <- sum(data[[paste0("cens_", var[i])]]) # number of left-censored
    # positions of left-censored:
    sdata[[paste0("Jcens_", varstan[i])]] <- as.array(seq_len(nrow(data))[data[[paste0("cens_", var[i])]]]) 
    sdata[[paste0("U_", varstan[i])]] <- lcl[i] # left-censoring limit
  }
  
  sdata
}

modify_stancode <- function(scode, var) {
  
  var <- str_remove_all(var, "_")
  
  for(i in seq_len(length(var))) {
    
    # modifications to data block:
    n_cens <- glue("int<lower=0> Ncens_{var[i]};  // number of left-censored")
    j_cens <- glue("int<lower=1> Jcens_{var[i]}[Ncens_{var[i]}];  // positions of left-censored")
    u <- glue("real U_{var[i]};  // left-censoring limit")
    # modifications to parameters block:
    y_cens <- glue("vector<upper=U_{var[i]}>[Ncens_{var[i]}] Ycens_{var[i]};  // estimated left-censored")
    # modifications to model block:
    yl <- glue("Yl_{var[i]}[Jcens_{var[i]}] = Ycens_{var[i]}; // add imputed left-censored values")
    
    scode <- scode %>%
      # modifications to data block:
      str_replace(
        "(data \\{\n(.|\n)*?)(?=\n\\})",
        paste(c("\\1", n_cens, j_cens, u), collapse = "\n  ")
      ) %>% 
      # modifications to parameters block:
      str_replace(
        "(parameters \\{\n(.|\n)*?)(?=\n\\})",
        paste(c("\\1\n  ", y_cens), collapse = "")
      ) %>% 
      # modifications to model block:
      str_replace(
        "(model \\{\n(.|\n)*?)(?=\n    mu_)",
        paste(c("\\1\n    ", yl), collapse = "")
      )
    
  }
  
  class(scode) <- "brmsmodel"
  
  scode
    
}
```

We'll also need a function to impute missing and censored values for prediction using the fitted model:

```{r functions-impute, echo=TRUE}
impute <- function(data, model, var, mi = NULL, cens = NULL, id = NULL) {
  
  varstan <- str_remove_all(var, "_")
  
  censored <- as_draws_df(model) %>% 
    as_tibble() %>% 
    select(starts_with(paste0("Ycens_", varstan))) %>% 
    t()
  
  if (!is.null(id)) {
    censored <- censored[id, ]
  }
  
  missing <- as_draws_df(model) %>% 
    as_tibble() %>% 
    select(starts_with(paste0("Ymi_", varstan))) %>% 
    t()
  
  ndraws <- ncol(censored)
  
  future_map(
    seq_len(ndraws),
    \(x) {
      data_imputed <- data
      if (!is.null(mi)) {data_imputed[mi, var] <- missing[, x]}
      if (!is.null(cens)) {data_imputed[cens, var] <- censored[, x]}
      data_imputed
    }, .progress = TRUE
  )
  
}
```

Now, we're ready to set up and run the model. First we need a formula that defines a model for both `y` and `x`. Since we don't have anything else to go on, the model for `x` is intercept only, but we should be able to do better than that in a real application. The `mi()` terms define missing values and how they're handled during model fitting---see the `brms` [vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html) for details.

```{r model-formula, echo=TRUE}
bform <- bf(y | mi() ~ mi(x_star)) + 
  bf(x_star | mi() ~ 1) +
  set_rescor(FALSE)
```

Next, we modify the data and code passed to Stan... 

```{r model-setup, echo=TRUE}
sdata <- make_standata(bform, data = data_train) %>% 
  modify_standata(data_train, lcl, "x_star")
scode <- make_stancode(bform, data = data_train) %>% 
  modify_stancode("x_star")
```

... and fit (and save) the model using `rstan::stan`.

```{r model-fit, echo=TRUE, eval=FALSE}
stanseed <- 1257
model_rstan <- stan(
  model_code = scode,
  data = sdata,
  sample_file = "model-censored-x", # save model as CSVs
  seed = stanseed
)
```

We can load the fitted model later using the following:

```{r model-load, echo=TRUE}
model_rstan <- read_stan_csv(list.files(pattern = "model-censored-x_"))
```

Now we can generate posterior predictions along a regular sequence of predictor values, using the posterior draws for the slope and intercept:

```{r predict-summarize, echo=TRUE}
post_draws <- as_draws_df(model_rstan) # posterior draws:

xnew <- seq(min(data$x), max(data$x), length.out = 25) # regular sequence of x-values

post_imp <- map(
  seq_len(nrow(post_draws)),
  \(x) {
    xnew * post_draws$`bsp_y[1]`[x] + post_draws$Intercept_y[x]
  }
) %>%
  do.call(rbind, .)
```

Here is the fitted model; the shaded region represents a 95% credible interval on the posterior expectation.

```{r plot-model}
preds <- tibble(
  x = xnew,
  yhat = apply(post_imp, 2, median),
  yhat_min = apply(post_imp, 2, \(x) quantile(x, .025)),
  yhat_max = apply(post_imp, 2, \(x) quantile(x, .975)),
  )

p1 + 
  geom_ribbon(
    data = preds,
    aes(x, ymin = yhat_min, ymax = yhat_max),
    alpha = .3,
    inherit.aes = FALSE
  ) +
  geom_line(
    data = preds,
    aes(x, yhat),
    inherit.aes = FALSE
  )
```

To estimate prediction performance for the training data, we first impute the censored and missing values. This generates a list of imputed datasets as long as the number of model interations (here, 4000).

```{r impute, eval=FALSE}
data_train_imputed <- impute(data_train, model_rstan, "x_star", sdata$Jmi_xstar, sdata$Jcens_xstar)
```

Then, we can calculate RMSE for the training data by generating predictions using the imputed datasets and the posterior draws of the model parameters:

```{r rmse-train}
rmse <- map2_dbl(
  seq_len(nrow(post_draws)),
  data_train_imputed,
  \(x, y) {
    yhat <- y$x_star * post_draws$`bsp_y[1]`[x] + post_draws$Intercept_y[x]
    sqrt(mean((y$y - yhat) ^ 2))
  }
)

tibble(mean_rmse = mean(rmse), q2.5 = quantile(rmse, .025), q97.5 = quantile(rmse, .975))
```

We can estimate out-of-sample error by imputing missing values and then generating posterior predictions. Imputating censored values, though, is slightly more complicated: since there is no guarantee that model predictions will fall below the censoring limit, we impute by refitting the model to the training set augmented by the censored observations from the test set.

```{r model-setup-combined}
data_combined <- bind_rows(data_train, data_test[data_test$cens_x_star, c("x_star", "cens_x_star")])
sdata_combined <- make_standata(bform, data = data_combined) %>% 
  modify_standata(data_combined, lcl, "x_star")
```

Fit:

```{r model-fit-combined}
model_rstan_combined <- stan(
  model_code = scode,
  data = sdata_combined,
  sample_file = "model-censored-x-combined",
  seed = stanseed,
  control = list(adapt_delta = .99)
)
```

```{r model-load-combined}
model_rstan_combined <- read_stan_csv(list.files(pattern = "model-censored-x-combined_"))
```

Impute censored:

```{r}

data_test_imputed <- impute(
  data = data_test,
  model = model_rstan_combined,
  var = "x_star",
  mi = NULL,
  cens = seq_len(nrow(data_test))[data_test$cens_x_star],
  id = 8:13
) %>% 
  # impute missings via posterior prediction:
  map2(
    seq_len(nrow(post_draws)),
    ~ .x %>% 
        mutate(
          x_star = if_else(
            is.na(x_star), 
            post_draws$Intercept_xstar[.y], 
            x_star
          )
        )
  )

```

The generate test predictions:

```{r}
preds_test <- map2(
  seq_len(nrow(post_draws)),
  data_test_imputed,
  ~ .y$x_star * post_draws$`bsp_y[1]`[.x] + post_draws$Intercept_y[.x]
)
```

Compare with the naive model. Compare only those predictions common to both models:

```{r}

model_naive <- brm(
  y ~ x_star,
  data = data_train,
  file = "model-naive",
  file_refit = "on_change"
)

# rmse:

do.call(rbind, preds_test)[,-4] %>% apply(1, \(x) sqrt(mean((x - data_test$y[-4]) ^ 2))) %>% ggdist::median_qi()
posterior_epred(model_naive, newdata = data_test)[ , -4] %>% apply(1, \(x) sqrt(mean((x - data_test$y[-4]) ^ 2))) %>% ggdist::median_qi()

preds_naive <- fitted(model_naive, newdata = data_test)

data_test %>% 
  mutate(
    # predictions from censored model:
    yhat_cens = apply(do.call(rbind, preds_test), 2, mean),
    ymin_cens = apply(do.call(rbind, preds_test), 2, \(x) quantile(x, .025)),
    ymax_cens = apply(do.call(rbind, preds_test), 2, \(x) quantile(x, .975)),
    # predictions from naive model:
    yhat_naive = preds_naive[, "Estimate"],
    ymin_naive = preds_naive[, "Q2.5"],
    ymax_naive = preds_naive[, "Q97.5"]
  ) %>% 
  pivot_longer(
    matches("y.+"),
    names_to = c(".value", "model"),
    names_pattern = "([^_]+)_([^_]+)"
  ) %>% 
  ggplot(aes(y, yhat)) + 
  facet_wrap(vars(model)) +
  geom_errorbar(aes(ymin = ymin, ymax = ymax)) +
  geom_abline() +
  geom_point() +
  geom_label(
    data = . %>% 
      group_by(model) %>% 
      summarize(
        rmse = sqrt(mean((y - yhat) ^ 2, na.rm = TRUE)),
        rmse = paste0("RMSE = ", round(rmse, 2))
      ),
    aes(x = -Inf, y = Inf, label = rmse),
    hjust = "inward", vjust = "inward",
    label.padding = unit(0.35, "lines"),
    label.size = 0
  )

```

And plot:

```{r}

```



<!-- extras -->

<!-- For post-processing, it's convenient to convert the model back into a `brms` object:  -->

```{r model-convert, eval=FALSE}
brmsmod <- brm(bform, data = data_train, empty = TRUE)
brmsmod$fit <- model_rstan
brmsmod <- rename_pars(brmsmod)
```

<!-- Make sure to use the `newdata` argument to generate predictions, though, since, `brms` doesn't know how to impute the censored predictor values---we'll have to do that ourselves. -->


<!-- Now we can generate posterior predictions from the model using our list of 4000 imputed datasets. The following code works, but it's slow---about 10 minutes of run time for this very simple model on my 2017 Macbook Pro. I'll eventually have to come up with a more efficient prediction method. -->

```{r predict-train, echo=TRUE, eval=FALSE}
post_imp <- future_map(
  seq_len(length(data_train_imputed)),
  ~ posterior_epred(brmsmod, newdata = data_train_imputed[[.x]], resp = "y", draw_ids = .x),
  .progress = TRUE
) %>% 
  do.call(rbind, .)
```

```{r predict-train-write, eval=FALSE}
write_csv(as_tibble(post_imp, .name_repair = "unique"), "posterior-predictions-training-data.csv")
```

```{r predict-train-read, eval=FALSE}
post_imp <- read_csv("posterior-predictions-training-data.csv")
```



